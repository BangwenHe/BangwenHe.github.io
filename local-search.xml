<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>llama2</title>
    <link href="/2024/03/14/llama2/"/>
    <url>/2024/03/14/llama2/</url>
    
    <content type="html"><![CDATA[<h1 id="LLaMa2"><a href="#LLaMa2" class="headerlink" title="LLaMa2"></a>LLaMa2</h1><h2 id="Transformer-Introduction"><a href="#Transformer-Introduction" class="headerlink" title="Transformer Introduction"></a>Transformer Introduction</h2><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>transformer 最主要的结构就是这张图：</p><p><img src="https://s2.loli.net/2024/03/13/WGRdjHr4MIOBKka.png" alt="Untitled.png"></p><p><img src="https://s2.loli.net/2024/03/13/KTXk7hmc9RPtYeA.png" alt="Untitled 1.png"></p><p>在纯 Encoder 或者纯 Decoder 的架构中，会变成只有左边的 transformer block 的结构，但是区别在于 Multi-Head Attention 是否存在 mask。Encoder-only 架构为了获取每一个 token 的完整上下文，因此没有对应的 mask；Decoder-only 架构为了让每一个 token 只能注意到它前面的 token，因此会存在一个从前往后的 mask，即生成的 $QK^T$ 矩阵（shape 为 $n_{tokens} \times n_{tokens}$）是一个下三角阵。</p><p><img src="https://s2.loli.net/2024/03/13/nXlmTSpWOjqhMaK.png" alt="llama2 的 QK^T 矩阵注意力输出，Decoder 架构，因此是一个下三角阵"></p><p>llama2 的 QK^T 矩阵注意力输出，Decoder 架构，因此是一个下三角阵</p><h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h3><p><a href="https://huggingface.co/blog/encoder-decoder">Transformer-based Encoder-Decoder Models</a></p><p>Transformer 是 Encoder-Decoder 结构的一种，这是为了解决在 NLP 中输入和输出长度不相同的问题，包括总结文本、生成新的文本。此前的 GRU 存在几个问题：1、Encoder 过长，会遗忘输入信息；2、Encoder 无法并行。（注意：Decoder 也无法并行，但是这是能够接受的，因为不知道输出的长度，因此只能每次输出一个 token）</p><p>为了一次 Encoder 推理捕获到全部的输入信息，transformer 将输入拼接起来，使用矩阵乘获取每个 token 之间的关系，然后通过 softmax 层变为系数矩阵，与输出的 V 矩阵相乘，得到了自注意力的输出。（为啥之前的 GRU 没有想到把所有的数据拼接，一次推理就能更新到最后的隐藏层的输出）</p><p>虽然预测推理时 Decoder 不能并行，但是训练时由于输出长度是已知的，所以使用了 Masked MHA 来进行并行化的训练。</p><p>输入和输出的 shape：$(N, \ N_{heads}, \ T, \ N_{embedding-size})$，$T$ 表示 time step，即输入 token 的数量；在 GPT 中，$T$ 随着输出不断变长。</p><p>注意：无论是 Encoder 还是 Decoder，它们输出的长度都是跟输入的长度是一样长的；在 Bert 中，输入是 masked sentence，输出是 reverse masked sentence，即对于被 mask 的单词的预测；在 GPT 中，它是一个生成任务，因此是通过不断地将输出拼接到输入中直到 token 为 EOS（End-Of-Sentence），由于输出和输入等长，因此输入是 $(BOS, x_1,x_2, \dots, x_{n-1})$，输出是 $(x_1,x_2,\cdots,x_n)$，最后一个 tensor 为当前输出的 token 的 embedding，通过 de-embedding 和 argmax 即可得到输出 token 的 id。</p><h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><p>GPT 是 Decoder-only 的架构，用于生成式任务。如果我们没有任何的输入和 Prompt，那么第一个 token 为 BOS（Begin-Of-Sentence）。</p><p>在 PyTorch 中，由于支持变长输入，所以输入的生成应该是如下形式：</p><ol><li>BPE 序列化 Prompt 成 Embedding，与 Positional Encoding 相加，得到初始输入<ol><li>如果没有 prompt，使用 BOS</li></ol></li><li>每次输入得到相同长度的输出，取最后一个 step 的 tensor 为输出 token 的 embedding</li></ol><h2 id="Llama2-c"><a href="#Llama2-c" class="headerlink" title="Llama2.c"></a>Llama2.c</h2><h3 id="LLama2"><a href="#LLama2" class="headerlink" title="LLama2"></a>LLama2</h3><p><a href="https://github.com/facebookresearch/llama/blob/main/llama/model.py"></a></p><p><img src="https://s2.loli.net/2024/03/13/RHgxY4FyuzshkZj.png" alt="llama2 对于 prompt 的处理"></p><p>llama2 对于 prompt 的处理</p><p>llama2 保持 transformer 架构的基本模式，即通过堆叠 tranformer block 来使得模型变深；与 CV 模型不同的是，CV 模型由于卷积模块的增加，feature map 会缩小，通道数会增加，因此不同 size 和 channel 的 block 不相同；但是 transformer block 是完全相同的，同一个类型的 block 从头用到尾，这也是输入和输出 shape 相同的原因。</p><p><img src="https://s2.loli.net/2024/03/13/bPtGswMhyzO1Bca.png" alt="Untitled 4.png"></p><p>llama2 的 transformer block 如下：</p><!-- <div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile&gt;&lt;diagram name=\&quot;Page-1\&quot; id=\&quot;IZTuIewHX-2reReE1Nqf\&quot;&gt;&lt;mxGraphModel dx=\&quot;1276\&quot; dy=\&quot;1163\&quot; grid=\&quot;0\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;0\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;850\&quot; pageHeight=\&quot;1100\&quot; background=\&quot;none\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;&lt;root&gt;&lt;mxCell id=\&quot;0\&quot;/&gt;&lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot;/&gt;&lt;mxCell id=\&quot;22\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;2\&quot; target=\&quot;21\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;2\&quot; value=\&quot;Masked Multi-Head Attention\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-101\&quot; y=\&quot;297\&quot; width=\&quot;160\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;14\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;5\&quot; target=\&quot;32\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;5\&quot; value=\&quot;RMSNorm\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-99\&quot; y=\&quot;440\&quot; width=\&quot;160\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;12\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0;exitDx=0;exitDy=0;entryX=0;entryY=0;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;6\&quot; target=\&quot;10\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;13\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=1;exitDx=0;exitDy=0;entryX=0;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;6\&quot; target=\&quot;8\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;&lt;mxPoint x=\&quot;89.00000000000011\&quot; y=\&quot;197\&quot; as=\&quot;targetPoint\&quot;/&gt;&lt;/mxGeometry&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;17\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;6\&quot; target=\&quot;26\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;&lt;mxPoint x=\&quot;-21\&quot; y=\&quot;90\&quot; as=\&quot;targetPoint\&quot;/&gt;&lt;/mxGeometry&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;6\&quot; value=\&quot;FFN\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-101\&quot; y=\&quot;109\&quot; width=\&quot;160\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;8\&quot; value=\&quot;SwiGeLU\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;91\&quot; y=\&quot;129.5\&quot; width=\&quot;160\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;10\&quot; value=\&quot;MatMul\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;91\&quot; y=\&quot;88.5\&quot; width=\&quot;160\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;19\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;18\&quot; target=\&quot;5\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;35\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;18\&quot; target=\&quot;21\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;&lt;Array as=\&quot;points\&quot;&gt;&lt;mxPoint x=\&quot;-19\&quot; y=\&quot;507\&quot;/&gt;&lt;mxPoint x=\&quot;-122\&quot; y=\&quot;507\&quot;/&gt;&lt;mxPoint x=\&quot;-122\&quot; y=\&quot;256\&quot;/&gt;&lt;/Array&gt;&lt;/mxGeometry&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;18\&quot; value=\&quot;X\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-47.5\&quot; y=\&quot;520\&quot; width=\&quot;57\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;25\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;20\&quot; target=\&quot;6\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;20\&quot; value=\&quot;RMSNorm\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-101\&quot; y=\&quot;173\&quot; width=\&quot;160\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;24\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;21\&quot; target=\&quot;20\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;31\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;21\&quot; target=\&quot;26\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;&lt;Array as=\&quot;points\&quot;&gt;&lt;mxPoint x=\&quot;-21\&quot; y=\&quot;233\&quot;/&gt;&lt;mxPoint x=\&quot;-122\&quot; y=\&quot;233\&quot;/&gt;&lt;mxPoint x=\&quot;-122\&quot; y=\&quot;72\&quot;/&gt;&lt;/Array&gt;&lt;/mxGeometry&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;21\&quot; value=\&quot;+\&quot; style=\&quot;ellipse;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-33.5\&quot; y=\&quot;243\&quot; width=\&quot;25\&quot; height=\&quot;25\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;29\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;entryX=0.5;entryY=1;entryDx=0;entryDy=0;\&quot; parent=\&quot;1\&quot; source=\&quot;26\&quot; target=\&quot;30\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;&lt;mxPoint x=\&quot;-20.850753495127947\&quot; y=\&quot;22.790911721539715\&quot; as=\&quot;targetPoint\&quot;/&gt;&lt;/mxGeometry&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;26\&quot; value=\&quot;+\&quot; style=\&quot;ellipse;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-33.5\&quot; y=\&quot;60\&quot; width=\&quot;25\&quot; height=\&quot;25\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;30\&quot; value=\&quot;Y\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-49.5\&quot; y=\&quot;-11\&quot; width=\&quot;57\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;33\&quot; style=\&quot;edgeStyle=none;curved=1;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=0;exitDx=0;exitDy=0;entryX=0.5;entryY=1;entryDx=0;entryDy=0;endArrow=open;startSize=14;endSize=14;sourcePerimeterSpacing=8;targetPerimeterSpacing=8;\&quot; parent=\&quot;1\&quot; source=\&quot;32\&quot; target=\&quot;2\&quot; edge=\&quot;1\&quot;&gt;&lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;mxCell id=\&quot;32\&quot; value=\&quot;Rotary Positional Encoding\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;&lt;mxGeometry x=\&quot;-99\&quot; y=\&quot;368\&quot; width=\&quot;160\&quot; height=\&quot;41\&quot; as=\&quot;geometry\&quot;/&gt;&lt;/mxCell&gt;&lt;/root&gt;&lt;/mxGraphModel&gt;&lt;/diagram&gt;&lt;/mxfile&gt;&quot;}"></div><script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script> --><p>由于 transformer 都是这种结构堆叠而成，因此许多推理框架都把结构硬编码放到代码中，包括下面的 LLama2.c。原始的 transformer 中间没有 Rotary Positional Encoding，这是 2021 年的 RoFormer 提出的，具体可参考 <strong><a href="https://kexue.fm/archives/8265">Transformer升级之路：2、博采众长的旋转式位置编码</a></strong> 。</p><h3 id="llama2-c"><a href="#llama2-c" class="headerlink" title="llama2.c"></a>llama2.c</h3><p><a href="https://github.com/karpathy/llama2.c">https://github.com/karpathy/llama2.c</a></p><p>llama2.c 根据上面的 llama2 的 transformer 的结构搭建了一个 transformer 的推理框架。主要介绍比较重要的几个模块。</p><p><strong>BPE Encoding</strong></p><p>部分分词法，能够得到英语中前后缀和词根，学习更多的数据。</p><p>算法如下：</p><ol><li>将句子拆分成字母，字母变为“子词”</li><li>合并其中两个相邻的子词成为一个新的子词，新子词的得分必须是最高的</li><li>不断迭代第二步，直到没有新的子词生成，即两个相邻子词生成的子词无法在词汇表中找到</li></ol><p><strong>Prompt</strong></p><p>prompt 可以理解为预先输入的一些单词，它们先走了一遍网络，初始化了权重，因此后面的输出才会根据 prompt 的提示输出。</p><p><strong>RMSNorm</strong></p><p>公式如下：</p><p>$$<br>X^\prime &#x3D; W \cdot \frac{X}{(\sum_{i&#x3D;1}^{n} X_i^2) &#x2F; n}<br>$$</p><p><strong>FFN</strong></p><p>公式如下：</p><p>$$<br>X^\prime &#x3D; W_2 \left ( (W_3 X) \cdot \text{SiLU} (W_1 X) \right )<br>$$</p><p>其中 $\text{SiLU}$ 函数如下：</p><p>$$<br>\text{SiLU}(x) &#x3D; x \cdot \sigma(x) &#x3D; \frac{x}{1 + \exp(-x)}<br>$$</p><p><strong>KV Cache</strong></p><p>由于 Decoder 每次推理生成一个 token，直到 token 为 EOS。每次推理得到的一个 token 会被视为新的输入再次执行推理，得到下一个 token。因此，之前生成的 Key 和 Value 矩阵可以保留下来，不需要再一次生成所有 token。</p><p>考虑到自注意力的公式为：</p><p>$$<br>\begin{align}<br>Q &amp;&#x3D; W_q X \<br>K &amp;&#x3D; W_k X \<br>V &amp;&#x3D; W_v X \<br>\text{Attn}(Q, K, V) &amp;&#x3D; Softmax \left ( \frac{QK^T}{\sqrt{n_{dim}}} \right )V<br>\end{align}<br>$$</p><p>假设有一行新的输入 $X_n$，经过投影后变为 $Q$ 的一行新的输入，经过自注意力后的输出与 $Q$ 的其它行无关，但是需要完整的 $K$ 和 $V$ 矩阵，因此我们需要 KV Cache。新的一行 KV Cache 就是新的输入 $X_n$ 经过投影之后的结果。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Transformer</tag>
      
      <tag>LLaMa2</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用zerotier实现免费上网</title>
    <link href="/2024/03/14/zerotier-free-web/"/>
    <url>/2024/03/14/zerotier-free-web/</url>
    
    <content type="html"><![CDATA[<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>本文提到的操作需要使用一台机器进行中转，这个机器可以是服务器，也可以是自己工位上的 Windows 机器。同时，这个机器上还必须打开了 Clash、V2Ray、SSR 之类的代理，并拥有一个端口号。</p><p>基本原理：假设我们在宿舍有一台笔记本，称为 Alice，简称为 A；在工位上有一台电脑，称为 Bob，简称为 B。A 只能访问校园网的网站，例如 csu.edu.cn；B 能够访问互联网，例如 baidu.com。那么，如果我们能够实现 A 的 https 请求通过 B 进行转发，然后再由 B 转发回 A，也就实现了让 A 上网。如图，</p><p><img src="https://s2.loli.net/2024/03/14/ykRW5ulcz8vHJQV.png" alt="image0.png"></p><p>从密码学角度来想，Bob 就是 Alice 和 Internet 的中间人，它可以获取二者通信的所有内容，所以我们要保证 Bob 一定是可信的。如果 Bob 不可信，我们还可以通过 https 进行通信，https 的所有信息都是被加密过的，如果 Bob 没有解密私钥，他就无法窃取机密信息。如何进行私钥分发，保证私钥的隐私性和不篡改性？一般是通过第三方的公证机构，所以 https 的认证证书是收费的。</p><p>要实现以上的连接，我们需要两个工具：</p><ul><li>实现 A 和 B 之间通信的工具，zerotier，这个是为了获取 IP。</li><li>B 为 A 做代理的软件，一般的代理软件都可以，例如 Clash、V2Ray、SSR，这个是为了获取代理端口。</li></ul><p>实际上，还是绕回了上一文中的，代理的两个关键变量：IP 和端口。</p><h2 id="Zerotier"><a href="#Zerotier" class="headerlink" title="Zerotier"></a>Zerotier</h2><h3 id="Zerotier-原理"><a href="#Zerotier-原理" class="headerlink" title="Zerotier 原理"></a>Zerotier 原理</h3><p>为什么我们需要 zerotier？我们可以简单地把 zerotier 理解为打洞软件。因为 IPv4 的 IP 池不够用，所以产生了 NAT 技术，这样可以让一个 IP 被很多人一起使用，从而保证了基于 IPv4 的互联网的可用性。但是，这也有缺点，很多人都使用同一个 IP，那么 P2P 通信很难实现，并且也很难追踪数据的来源（所以知乎显示的 IP 都是省级的）。</p><p>在 NAT 环境下，为了实现 P2P 通信，可以使用的一个技术叫做打洞。具体实现原理我不是非常清楚，但是可以简要地说明一下。假设 A（192.168.1.*） 和 B（172.16.6.*） 之间有一个路由器 C，但是路由器两边的 IP 不同。B 可以向 A 发送数据包，但是 A 不能向 B 发送数据包，存在一个通信的 gap。NAT 技术的原理是二者进行通信的时候，对于公用的 IP，路由器会记录源IP、源端口、目标 IP、目标端口，本来 NAT 是一个 IP 层的技术，但是为了解决通信的问题，它使用了 TCP 层的技术。如果我们能够确保 C 上的一个端口永远是被 B 所占用的，那么 A 可以向 C 的这个端口发送数据包，这个数据包就可以保证被 B 所接收，这样就实现了二者的 P2P 通信。<br>在校园网环境下，实际的路由情况比较复杂，我的测试结果是从宿舍到实验室，中间应该经过了 5 层路由。</p><h3 id="Tracert-测试"><a href="#Tracert-测试" class="headerlink" title="Tracert 测试"></a>Tracert 测试</h3><p>为了保证宿舍和实验室之间是存在路由关系，并且保证是在学校内网中的。我们需要先测试一下，使用的工具是 Windows 自带的 tracert 工具，在 Linux 上是 traceroute。</p><p>首先，在宿舍，连接 CSU-Student，一定要使用 WiFi 连接，因为我发现 WiFi 连接之后，打洞的延迟很低，在 4~10ms 左右波动，而网线连接的延迟很高，基本在 300ms 以上。</p><p><img src="https://s2.loli.net/2024/03/14/sp7N1ubTx2QF4D9.png" alt="image1.png"></p><p>连接 WiFi 之后，打开浏览器进行认证，随便访问一个网站就会自动跳转到认证网页了。注意：此时浏览器代理或者全局代理软件都不能打开，否则流量会被劫持，从而被转发到代理网站上，而代理网站不能连接，也就既不能认证又不能上网。认证网页，记得选择校园网，校园网可以免费登录：</p><p><img src="https://s2.loli.net/2024/03/14/YDt1TSR73JHeNj9.png" alt="image2.png"></p><p>认证成功：</p><p><img src="https://s2.loli.net/2024/03/14/psIiybme18SOGX7.png" alt="image3.png"></p><p>认证成功之后，可以访问各种学校网站，例如官网 csu.edu.cn、计算机学院官网 cse.csu.edu.cn。所以，我们要使用 tracert 测试宿舍机器到 csu.edu.cn 的连通性。如图，</p><p><img src="https://s2.loli.net/2024/03/14/OuEfmLRhUSBlnbg.png" alt="image4.png"></p><p><img src="https://s2.loli.net/2024/03/14/AZ4SxMUGONXkVhe.png" alt="image5.png"></p><p>第一张图是在宿舍环境下的 tracert 测试，第二张图是在实验室环境下的 tracert 测试。可以发现，两台机器都访问了同一个 IP，172.30.200.2，说明这两台机器之间是存在路由连接的，因此我们通过打洞实现二者之间的 P2P 通信是可行的。</p><h3 id="Zerotier-安装"><a href="#Zerotier-安装" class="headerlink" title="Zerotier 安装"></a>Zerotier 安装</h3><p>到 zerotier 官网下载，直接安装即可。安装之后的界面：</p><p><img src="https://s2.loli.net/2024/03/14/MpX6iQAekT9CF4g.png" alt="image6.png"></p><p>可以看到，右下角多了一个正在运行的软件。a84ac5c10a851cf8 是我自己创建的虚拟局域网。如果需要加入一个虚拟局域网，我们首先需要自己创建一个 zerotier 账号，并且创建一个虚拟局域网。当创建好虚拟局域网之后，在每台设备上加入组建的虚拟局域网。两台机器都加入之后，还需要到 zerotier 控制台中进行额外的配置。如图所示，</p><p><img src="https://s2.loli.net/2024/03/14/LChtn1Ra8KqJ5BH.png" alt="image7.png"></p><p>因为默认创建的虚拟局域网是私有的，所以必须点上 Auth 的按钮。同时，还可以给它分配一个 IP 和名称。做好以上步骤之后，就可以 zerotier 帮我们实现自动打洞了。</p><p>注意，Windows 会默认打开防火墙，所以 ping 是会失败的。最简单的是在被连接的电脑 B 上关闭防火墙，虽然比较危险，但是在实验室内也只发现了一次挖矿病毒，所以不需要特别在意。如果担心防火墙打开的风险，也可以在测试完之后关闭。</p><h3 id="测试连通性"><a href="#测试连通性" class="headerlink" title="测试连通性"></a>测试连通性</h3><p>由于 Windows 关闭了 ICMP 端口的连接，所以是无法直接 ping 通的。最简单的方法是关闭防火墙，也可以通过 控制面板 -&gt; 系统和安全 -&gt; Windows Defender 防火墙 -&gt; 高级设置 -&gt; 入站规则 进行配置入站规则，将四个核心网络诊断 - ICMP回显请求打开（右键，启用规则）。如图，</p><p><img src="https://s2.loli.net/2024/03/14/2ZSblMydnKuXQeG.png" alt="image8.png"></p><p>本来是不可以 ping 通的，但是在打开之后，就可以 ping 通了。</p><p><img src="https://s2.loli.net/2024/03/14/v78HdnlK3t6ZEwA.png" alt="image9.png"></p><p>测试的方式：打开控制台，输入 ping ${IP_of_A} ，如上图所示。如果在已经打开了 ICMP 的情况下，并且延迟正常显示，那么就说明两台机器之间可以正常通信了。</p><h2 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h2><p>代理就比较简单了，只需要提供一个端口即可。我使用的是 Clash，像上文提到的那样，找到 Clash 的代理端口，那么这个就是我们需要连接的代理端口了。</p><p>[图片]</p><p>如果是 V2ray 和 SSR，在设置里面也有。</p><p>在拥有了 IP 和端口号之后，我们就可以把 B 视为我们的代理服务器，让 B 代替我们进行互联网访问，并且，由于这个端口号上监听的程序本来就是一个代理软件，我们的流量就可以自动地走代理程序，可以直接访问谷歌。<br>注意：代理的端口号也是被 Windows 防火墙关闭了的，所以，必须在入站规则中打开对应的代理端口，否则也是无法连接的。</p><h3 id="Clash-配置"><a href="#Clash-配置" class="headerlink" title="Clash 配置"></a>Clash 配置</h3><p>下载我的 clash 配置文件，并手动修改对应的 IP 和端口号。修改完成之后，在 profiles 界面导入配置文件。并且，点击右图中的测速按钮，可以得到延迟的大小。如果这个延迟是绿色的，那么就说明网络连接成功，可以使用。打开 General 界面的 System Proxy 按钮，这时整个系统的代理都会被 Clash 劫持。打开浏览器，就可以正常上网了。</p><p>配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">proxies:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">http</span><br>    <span class="hljs-attr">type:</span> <span class="hljs-string">http</span><br>    <span class="hljs-attr">server:</span> <span class="hljs-number">10.147</span><span class="hljs-number">.20</span><span class="hljs-number">.2</span>  <span class="hljs-comment"># set your virtual remote IP provided by zerotier</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">7890</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">backup</span><br>    <span class="hljs-attr">type:</span> <span class="hljs-string">http</span><br>    <span class="hljs-attr">server:</span> <span class="hljs-number">10.147</span><span class="hljs-number">.20</span><span class="hljs-number">.3</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">7890</span><br><br><span class="hljs-attr">proxy-groups:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Proxy</span><br>    <span class="hljs-attr">type:</span> <span class="hljs-string">select</span><br>    <span class="hljs-attr">proxies:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">http</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">backup</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Game</span><br>    <span class="hljs-attr">type:</span> <span class="hljs-string">select</span><br>    <span class="hljs-attr">proxies:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">http</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-string">backup</span><br>      <br><br><span class="hljs-attr">rules:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">DOMAIN-SUFFIX,steampowered,Game</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">MATCH,</span> <span class="hljs-string">Proxy</span>  <span class="hljs-comment"># all connections fallback to remote</span><br></code></pre></td></tr></table></figure><h3 id="System-Proxy-配置"><a href="#System-Proxy-配置" class="headerlink" title="System Proxy 配置"></a>System Proxy 配置</h3><p>如果不想使用 Clash 做代理软件，也可以直接用 Windows 自带的代理设置，实际上，各种代理软件都是修改这个设置来劫持系统流量的。在 设置 -&gt; 网络和Internet -&gt; 代理 中，选择 手动设置代理 ，并填入对应的 B 机器的虚拟局域网的 IP 和端口，就可以正常代理，并上网了。</p><h3 id="测速"><a href="#测速" class="headerlink" title="测速"></a>测速</h3><p><img src="https://s2.loli.net/2024/03/14/B6g8pur2zlCaoDO.png" alt="image12.png"></p><p>基本上可以跑满实验室的带宽。</p><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ol><li>只有 PC 可用，而且只能用在 WiFi 上</li><li>Zerotier 打洞不稳定，截至 2022.09.29，我用了十几天，有一天存在断流，网速只有 20Mbps</li></ol><p>参考<br><a href="https://kenvix.com/post/use-school-network-without-paying-guide/">https://kenvix.com/post/use-school-network-without-paying-guide/</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Caffe2 中 float32 与 float16 的转换</title>
    <link href="/2024/03/12/float32-to-float16-caffe/"/>
    <url>/2024/03/12/float32-to-float16-caffe/</url>
    
    <content type="html"><![CDATA[<p class="note note-primary">generated by Copilot and translated by DeepL</p><p>这段 C++ 代码是一个名为 <code>cpu_float2half_rn</code> 的函数，用于将单精度浮点数（32 位）转换为半精度浮点数（16 位）。该函数将浮点数作为参数，并返回一个 float16 对象。转换过程涉及多个步骤，包括处理 NaN 和无穷大等特殊情况，从输入中提取符号、指数和尾数，以及四舍五入到最接近的偶数。</p><ol><li>函数首先声明了一个 float16 对象 ret，用于保存转换结果。然后使用 <code>static_assert</code> 来确保无符号 int 和 float 的大小相同。这一点很重要，因为函数使用位操作来处理 float 的二进制表示，这需要将 float 解释为无符号整数。</li><li>函数使用 <code>reinterpret_cast</code> 获得指向 float 二进制表示形式的指针。然后创建该二进制表示的副本，屏蔽符号位，得到浮点数 u 的绝对值。</li><li>函数会处理特殊情况。如果 u 大于 0x7f800000，输入值为 NaN，函数会将结果设置为特殊的 NaN 值并返回。如果 u 大于 0x477fefff 或小于 0x33000001，则输入值为无穷大或零，函数会将结果设置为适当的值并返回。</li><li>函数从浮点数的二进制表示中提取符号、指数和尾数。符号是最高位，指数是接下来的 8 位，尾数是剩余的 23 位。</li><li>函数会调整指数和尾数，以适应较小的半精度格式。如果指数大于 0x70，则从指数中减去 0x70，并将尾数右移 13 位。否则，它会将指数设为 0，设置尾数的最高位，并计算尾数右移的位数。</li><li>函数将尾数四舍五入为最接近的偶数。计算移位后的最小有效位及其以下的位数。如果最小有效位以下的位数大于一半或正好是一半，且最小有效位为 1，则尾数递增。如果尾数溢出，则递增指数并重置尾数。</li><li>函数将符号、指数和尾数合并为半精度格式，并将结果存储在 ret 中。然后返回 ret。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">inline</span> float16 <span class="hljs-title">cpu_float2half_rn</span><span class="hljs-params">(<span class="hljs-type">float</span> f)</span> </span>&#123;<br>  float16 ret;<br><br>  <span class="hljs-built_in">static_assert</span>(<br>      <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">unsigned</span> <span class="hljs-type">int</span>) == <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>),<br>      <span class="hljs-string">&quot;Programming error sizeof(unsigned int) != sizeof(float)&quot;</span>);<br><br>  <span class="hljs-type">unsigned</span>* xp = <span class="hljs-built_in">reinterpret_cast</span>&lt;<span class="hljs-type">unsigned</span> <span class="hljs-type">int</span>*&gt;(&amp;f);<br>  <span class="hljs-type">unsigned</span> x = *xp;<br>  <span class="hljs-type">unsigned</span> u = (x &amp; <span class="hljs-number">0x7fffffff</span>), remainder, shift, lsb, lsb_s1, lsb_m1;<br>  <span class="hljs-type">unsigned</span> sign, exponent, mantissa;<br><br>  <span class="hljs-comment">// Get rid of +NaN/-NaN case first.</span><br>  <span class="hljs-keyword">if</span> (u &gt; <span class="hljs-number">0x7f800000</span>) &#123;<br>    ret.x = <span class="hljs-number">0x7fff</span>U;<br>    <span class="hljs-keyword">return</span> ret;<br>  &#125;<br><br>  sign = ((x &gt;&gt; <span class="hljs-number">16</span>) &amp; <span class="hljs-number">0x8000</span>);<br><br>  <span class="hljs-comment">// Get rid of +Inf/-Inf, +0/-0.</span><br>  <span class="hljs-keyword">if</span> (u &gt; <span class="hljs-number">0x477fefff</span>) &#123;<br>    ret.x = sign | <span class="hljs-number">0x7c00</span>U;<br>    <span class="hljs-keyword">return</span> ret;<br>  &#125;<br>  <span class="hljs-keyword">if</span> (u &lt; <span class="hljs-number">0x33000001</span>) &#123;<br>    ret.x = (sign | <span class="hljs-number">0x0000</span>);<br>    <span class="hljs-keyword">return</span> ret;<br>  &#125;<br><br>  exponent = ((u &gt;&gt; <span class="hljs-number">23</span>) &amp; <span class="hljs-number">0xff</span>);<br>  mantissa = (u &amp; <span class="hljs-number">0x7fffff</span>);<br><br>  <span class="hljs-keyword">if</span> (exponent &gt; <span class="hljs-number">0x70</span>) &#123;<br>    shift = <span class="hljs-number">13</span>;<br>    exponent -= <span class="hljs-number">0x70</span>;<br>  &#125; <span class="hljs-keyword">else</span> &#123;<br>    shift = <span class="hljs-number">0x7e</span> - exponent;<br>    exponent = <span class="hljs-number">0</span>;<br>    mantissa |= <span class="hljs-number">0x800000</span>;<br>  &#125;<br>  lsb = (<span class="hljs-number">1</span> &lt;&lt; shift);<br>  lsb_s1 = (lsb &gt;&gt; <span class="hljs-number">1</span>);<br>  lsb_m1 = (lsb - <span class="hljs-number">1</span>);<br><br>  <span class="hljs-comment">// Round to nearest even.</span><br>  remainder = (mantissa &amp; lsb_m1);<br>  mantissa &gt;&gt;= shift;<br>  <span class="hljs-keyword">if</span> (remainder &gt; lsb_s1 || (remainder == lsb_s1 &amp;&amp; (mantissa &amp; <span class="hljs-number">0x1</span>))) &#123;<br>    ++mantissa;<br>    <span class="hljs-keyword">if</span> (!(mantissa &amp; <span class="hljs-number">0x3ff</span>)) &#123;<br>      ++exponent;<br>      mantissa = <span class="hljs-number">0</span>;<br>    &#125;<br>  &#125;<br><br>  ret.x = (sign | (exponent &lt;&lt; <span class="hljs-number">10</span>) | mantissa);<br><br>  <span class="hljs-keyword">return</span> ret;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ORT 模型部署</title>
    <link href="/2024/03/12/Deploy-ORT-model/"/>
    <url>/2024/03/12/Deploy-ORT-model/</url>
    
    <content type="html"><![CDATA[<h1 id="Deploy-ORT-model"><a href="#Deploy-ORT-model" class="headerlink" title="Deploy ORT model"></a>Deploy ORT model</h1><p><a href="https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/imagenet_v2/mobilenet.ipynb">onnxruntime-inference-examples&#x2F;mobilenet.ipynb at main · microsoft&#x2F;onnxruntime-inference-examples</a></p><p><a href="https://zhuanlan.zhihu.com/p/128974102">详细记录YOLACT实例分割ncnn实现</a></p><p>真正部署模型，不应该把后处理包括在模型推理中，这会影响模型在GPU上的部署，性能也不一定会好。这里的后处理，不仅仅是<code>model(input)</code> 之后的，也可以是作者放在模型推理过程中，但是实际上可以归为后处理的部分。</p><p>判断函数是否在ONNX trace的过程中：<code>torch.onnx.is_in_onnx_export()</code> 。</p><p><a href="https://www.notion.so/ONNX-to-TF-6049662a9e71472aaff545676a19050c">ONNX to TF</a></p><h2 id="Torch-to-ONNX"><a href="#Torch-to-ONNX" class="headerlink" title="Torch to ONNX"></a>Torch to ONNX</h2><ol><li>torch不支持<code>F.grid_sample</code> 算子。从<a href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">ONNX支持的算子列表</a>来看，<code>opset=16</code> 时，可以直接使用<code>grid_sampler</code> 而不需要手动设置符号函数。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># https://github.com/pytorch/pytorch/issues/27212#issuecomment-1059773074</span><br><span class="hljs-comment"># https://gist.github.com/daigo0927/8c8b3005cffb61983e80ceab6c1f2274</span><br><span class="hljs-comment"># https://github.com/onnx/onnx/pull/3557</span><br><br><span class="hljs-keyword">from</span> torch.onnx <span class="hljs-keyword">import</span> register_custom_op_symbolic<br><span class="hljs-keyword">import</span> torch.onnx.symbolic_helper <span class="hljs-keyword">as</span> sym_help<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">grid_sampler</span>(<span class="hljs-params">g, <span class="hljs-built_in">input</span>, grid, mode, padding_mode, align_corners</span>):<br>    <span class="hljs-comment"># mode</span><br>    <span class="hljs-comment">#   &#x27;bilinear&#x27;      : onnx::Constant[value=&#123;0&#125;]</span><br>    <span class="hljs-comment">#   &#x27;nearest&#x27;       : onnx::Constant[value=&#123;1&#125;]</span><br>    <span class="hljs-comment">#   &#x27;bicubic&#x27;       : onnx::Constant[value=&#123;2&#125;]</span><br>    <span class="hljs-comment"># padding_mode</span><br>    <span class="hljs-comment">#   &#x27;zeros&#x27;         : onnx::Constant[value=&#123;0&#125;]</span><br>    <span class="hljs-comment">#   &#x27;border&#x27;        : onnx::Constant[value=&#123;1&#125;]</span><br>    <span class="hljs-comment">#   &#x27;reflection&#x27;    : onnx::Constant[value=&#123;2&#125;]</span><br>    mode = sym_help._maybe_get_const(mode, <span class="hljs-string">&quot;i&quot;</span>)<br>    padding_mode = sym_help._maybe_get_const(padding_mode, <span class="hljs-string">&quot;i&quot;</span>)<br>    mode_str = [<span class="hljs-string">&#x27;bilinear&#x27;</span>, <span class="hljs-string">&#x27;nearest&#x27;</span>, <span class="hljs-string">&#x27;bicubic&#x27;</span>][mode]<br>    padding_mode_str = [<span class="hljs-string">&#x27;zeros&#x27;</span>, <span class="hljs-string">&#x27;border&#x27;</span>, <span class="hljs-string">&#x27;reflection&#x27;</span>][padding_mode]<br>    align_corners = <span class="hljs-built_in">int</span>(sym_help._maybe_get_const(align_corners, <span class="hljs-string">&quot;b&quot;</span>))<br><br>    <span class="hljs-keyword">return</span> g.op(<span class="hljs-string">&quot;com.microsoft::GridSample&quot;</span>, <span class="hljs-built_in">input</span>, grid,<br>                mode_s=mode_str,<br>                padding_mode_s=padding_mode_str,<br>                align_corners_i=align_corners)<br>    <br>register_custom_op_symbolic(<span class="hljs-string">&#x27;::grid_sampler&#x27;</span>, grid_sampler, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><ol start="2"><li><code>_DCNv2</code> 完全不受支持。它是用CUDA编译的，无论是自带的DCNv2还是mmcv的DCNv2。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@staticmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">symbolic</span>(<span class="hljs-params"></span><br><span class="hljs-params">    g, <span class="hljs-built_in">input</span>, offset, mask, weight, bias, stride, padding, dilation, deformable_groups</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-keyword">from</span> torch.nn.modules.utils <span class="hljs-keyword">import</span> _pair<br><br>    stride = _pair(stride)<br>    padding = _pair(padding)<br>    dilation = _pair(dilation)<br>    <span class="hljs-comment"># as of trt 7, the dcn operation will be translated again by modifying the onnx file</span><br>    <span class="hljs-comment"># so the exporting code is kept to resemble the forward()</span><br>    <span class="hljs-keyword">return</span> g.op(<br>        <span class="hljs-string">&quot;custom_domain::_DCNv2&quot;</span>,<br>        <span class="hljs-built_in">input</span>,<br>        offset,<br>        mask,<br>        weight,<br>        bias,<br>        stride_i=stride,<br>        padding_i=padding,<br>        dilation_i=dilation,<br>        deformable_groups_i=deformable_groups,<br>    )<br></code></pre></td></tr></table></figure><p>在这里可以注册一个符号函数(<code>symbolic_function</code>)来自己手动通过cpp实现DCNv2，但是我其实并不确定它能不能运行到mobile端。最上面导出NCNN的方法其实说过，可以跳过它，就是不用它，换到一个Conv层就行。在实现层面上，参考下面的模型变量key：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">5</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.weight&#x27;</span><br><span class="hljs-number">6</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.bias&#x27;</span><br><span class="hljs-number">7</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.conv_offset_mask.weight&#x27;</span><br><span class="hljs-number">8</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.conv_offset_mask.bias&#x27;</span><br></code></pre></td></tr></table></figure><p>它自带了一个conv，同时还有一个conv_offset_mask来学习stride的微小偏移量，这使得我们可以不使用后面的conv_offset_mask，只使用conv本身的权重来推理。不过这样会丢掉一些性能。下面这个用的是TensorRT推理，所以可以支持CUDA。DCNv2的实现中，其中一个就是参考了这个repo的。它这里面实现了自定义的算子，并且用符号函数把算子和onnx计算图联系到了一起。</p><p><a href="https://github.com/CaoWGG/TensorRT-CenterNet/blob/master/readme/ctdet2onnx.md">TensorRT-CenterNet&#x2F;ctdet2onnx.md at master · CaoWGG&#x2F;TensorRT-CenterNet</a></p><ol start="3"><li><code>Floating point exception (core dumped)</code> 。这个报错出现地非常地无厘头，因为它没有任何报错栈，只是单纯地显示它出错了。如果用torch1.9，会有更多的报错信息，虽然基本上也看不了。截图如下：</li></ol><p><img src="https://s2.loli.net/2024/03/12/tpTenzEYO7GFwB6.png" alt="raw_error.png"></p><p><a href="https://discuss.pytorch.org/t/dealing-with-floating-point-exceptions/51882/3">Dealing with floating point exceptions</a></p><p>通过<code>gdb --args python [finetune.py](http://finetune.py) $&#123;params&#125;</code> ，可以查看到更多的报错信息，截图如下：</p><p><img src="https://s2.loli.net/2024/03/12/oIicYZWQ9tR1Av7.png" alt="gdb_callstack.png"></p><h2 id="ONNX-to-ORT"><a href="#ONNX-to-ORT" class="headerlink" title="ONNX to ORT"></a>ONNX to ORT</h2><ol><li>除法算子导致张量除法的datatype不对应。<code>onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from /media/tclab/980Pro/users/bangwhe/e2ec/gcn.onnx failed:Type Error: Type parameter (T) of Optype (Div) bound to different types (tensor(double) and tensor(float) in node (Div_337).</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原代码</span><br>img_poly[..., <span class="hljs-number">0</span>] = img_poly[..., <span class="hljs-number">0</span>] / (w / <span class="hljs-number">2.</span>) - <span class="hljs-number">1</span><br>img_poly[..., <span class="hljs-number">1</span>] = img_poly[..., <span class="hljs-number">1</span>] / (h / <span class="hljs-number">2.</span>) - <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 修改后的代码，w先做除法</span><br>img_poly[..., <span class="hljs-number">0</span>] = img_poly[..., <span class="hljs-number">0</span>] / w * <span class="hljs-number">2.</span> - <span class="hljs-number">1</span><br>img_poly[..., <span class="hljs-number">1</span>] = img_poly[..., <span class="hljs-number">1</span>] / h * <span class="hljs-number">2.</span> - <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><ol start="2"><li>循环中的<code>==</code> 生成了bool，可能不受支持。<code>onnxruntime.capi.onnxruntime_pybind11_state.InvalidGraph: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from /media/tclab/980Pro/users/bangwhe/e2ec/gcn.onnx failed:This is an invalid model. Type Error: Type &#39;tensor(bool)&#39; of input parameter (763) of operator (ScatterND) in node (ScatterND_543) is invalid.</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原代码</span><br>batch_size = cnn_feature.size(<span class="hljs-number">0</span>)<br>gcn_feature = torch.zeros([img_poly.size(<span class="hljs-number">0</span>), cnn_feature.size(<span class="hljs-number">1</span>), img_poly.size(<span class="hljs-number">1</span>)]).to(img_poly.device)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>    poly = img_poly[ind == i].unsqueeze(<span class="hljs-number">0</span>)<br>    feature = torch.nn.functional.grid_sample(cnn_feature[i:i+<span class="hljs-number">1</span>], poly)[<span class="hljs-number">0</span>].permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>    gcn_feature[ind == i] = feature<br><br><span class="hljs-comment"># 修改后的代码</span><br>gcn_feature[<span class="hljs-number">0</span>] = torch.nn.functional.grid_sample(cnn_feature[<span class="hljs-number">0</span>:<span class="hljs-number">1</span>], img_poly[<span class="hljs-number">0</span>:<span class="hljs-number">1</span>].unsqueeze(<span class="hljs-number">0</span>))[<span class="hljs-number">0</span>].permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="ORT-NNAPI"><a href="#ORT-NNAPI" class="headerlink" title="ORT NNAPI"></a>ORT NNAPI</h2><p><a href="https://github.com/BangwenHe/ORTSegDemo">https://github.com/BangwenHe/ORTSegDemo</a></p><p><a href="https://github.com/microsoft/onnxruntime/blob/master/java/src/test/java/sample/ScoreMNIST.java">onnxruntime&#x2F;ScoreMNIST.java at master · microsoft&#x2F;onnxruntime</a></p><p><a href="https://github.com/microsoft/onnxruntime-inference-examples/blob/main/mobile/examples/image_classification/android/app/src/main/java/ai/onnxruntime/example/imageclassifier/ImageUtil.kt">onnxruntime-inference-examples&#x2F;ImageUtil.kt at main · microsoft&#x2F;onnxruntime-inference-examples</a></p><p>导出成onnx或者ort模型后，部署到手机上肯定是会支持CPU的，但是是否支持GPU还需要看模型的支持或者NNAPI的支持。增加CPU的线程数可以直接使用<code>mSessionOptions.setIntraOpNumThreads(4);</code> ，肯定是支持的。</p><ol><li>shape错误，应该是slice（切片）算子不受到NNAPI的支持。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">W/System.err: ai.onnxruntime.OrtException: Error code - ORT_FAIL - <br>message: model_builder.cc:<span class="hljs-number">374</span> RegisterModelOutputs shape_proto cannot be null <span class="hljs-keyword">for</span> output: <span class="hljs-number">217</span><br></code></pre></td></tr></table></figure><ol start="2"><li>我认为我导出的这个e2ec或者是snake不能运行到mobile端的GPU上应该是因为添加了太多的逻辑运算，而不仅仅是Conv、BN、ReLU这些算子，例如NonZero，还有一些多维取值算子，例如ScatterND、Gather等。这些逻辑算子和取值算子对于逻辑运算单元少的GPU来说，并行起来是非常困难的。所以优化的角度应该是把类似后处理的逻辑运算给提取出来，只保留大部分的backbone，手动从heatmap中提取关键点的位置信息，<a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite">posenet</a>输出的就是heatmap，需要手动做提取。</li></ol><h2 id="Build-ONNXRuntime-From-Source"><a href="#Build-ONNXRuntime-From-Source" class="headerlink" title="Build ONNXRuntime From Source"></a>Build ONNXRuntime From Source</h2><p><a href="https://onnxruntime.ai/docs/build/android.html">Build for Android</a></p><p>要求cmake 3.18+，android SDK需要手动通过sdkmanager下载。加上<code>--build_java</code> 会很慢，所以先删掉。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> PATH=/mnt/tbdisk/bangwhe/experiments/cmake-3.20.6-linux-x86_64/bin/:<span class="hljs-variable">$PATH</span><br><br>./build.sh --android --build_nnapi\<br>--android_sdk_path /mnt/tbdisk/bangwhe/env/Android/platforms/android-29/ \<br>--android_ndk_path /mnt/tbdisk/bangwhe/env/android-ndk-r20b/<br></code></pre></td></tr></table></figure><p>编译得到的so库都保存在build文件夹中，下面有子文件夹，分别保存安卓端和Linux桌面端。</p><h2 id="OnnxRuntime-C-Impl"><a href="#OnnxRuntime-C-Impl" class="headerlink" title="OnnxRuntime C++ Impl"></a>OnnxRuntime C++ Impl</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p><a href="https://github.com/microsoft/onnxruntime-inference-examples/blob/main/c_cxx/model-explorer/model-explorer.cpp">onnxruntime-inference-examples&#x2F;model-explorer.cpp at main · microsoft&#x2F;onnxruntime-inference-examples</a></p><p><code>error: use of deleted function ‘Ort::Experimental::Session::Session(const Ort::Experimental::Session&amp;)</code> ：函数被删除了，后面有debug信息。这个session不能通过函数传到其它的函数里，从而被调用。<code>Ort::Value</code> 也是一样的，但是通过<code>std::move</code> 将其变成右值，就可以传到<code>vector</code> 中。（Why？）</p><p>vscode添加索引路径：command palette → C&#x2F;C++: Edit Configurations (JSON)</p><p><code>2022-08-15 10:50:20.544504982 [W:onnxruntime:, graph.cc:1220 Graph] Initializer 1894 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.</code> initializer 1894 出现在图形输入中，不会被视为常量值&#x2F;权重，可能会阻止一些类似常量折叠的优化，可以使用<code>onnxruntime/tools/python/remove_initializer_from_input.py</code> 来进行移除操作。移除前的推理时间是45ms，移除后是19ms，提升还是比较大的。</p><p><code>cv:dnn:blobFromImage</code> 不太好用，经常把320x320的Mat变成3x1的Mat。（Why？）</p><p>OpenCV的<code>Mat</code> 类和ORT的<code>Ort::Value</code> 类都可以得到它们的数据指针，方法分别是：</p><ul><li><code>[Mat.data](http://Mat.data)</code> 可以直接得到数据的指针，但是更常用的是<code>Mat.at&lt;DataType&gt;()</code></li><li><code>Ort::Value::GetTensorMutableData&lt;DataType&gt;()</code> ，得到<code>Ort::Value</code> 保存的张量的数据指针</li><li>通过这两个指针，我们可以得到预处理得到的结果和输入网络前的张量的大小。如果需要对比预处理是否相同，可以自己读指针，然后打印数据出来跟python版的数据做对比。</li></ul><p><code>terminate called after throwing an instance of &#39;Ort::Exception&#39; what():  not enough space: expected 1228800, got 409600</code> ：明显的报错，空间不够。这是因为传入的<code>p_data_element_count</code> 和<code>*shape</code> 的乘积大小不相同，所以导致了报错，传入的参数为<code>102400</code> 和<code>307200</code> ，再乘上float类型占用的4byte，有<code>409600</code> 和<code>1228800</code> ，所以商（3）代表少了三个通道，给<code>p_data_element_count</code> 乘上3即可。</p><p><code>incomplete type is not allowed</code> ：需要先定义一个type，才能在后续使用这个标识符的时候知道它是啥。这里报错是因为OrtSession这个类（标识符）不存在，需要变成指针类型才可以，即OrtSession*类型。</p><p><a href="https://www.notion.so/02-32a4f1797c1b46758f2a01ed714970bf">02</a></p><p><code>error: use of deleted function ‘Ort::Session::Session(const Ort::Session&amp;)’</code> ：使用了被删除的拷贝构造函数，所以不能把它当作变量传到某一个函数中，编译器就会报错（为什么会删除呢？参考我的02）。那我们总不可能不用函数吧？在<a href="https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/session/onnxruntime_c_api.h#L3500">3501</a>行，注释中说：</p><blockquote><p>Prevent users from accidentally copying the API structure, it should always be passed as a pointer.</p></blockquote><p>所以我们应该传入一个指针。从类名往上溯源，发现一个<code>Base</code> 类，它有一个函数：<code>operator T*() &#123; return p_; &#125;</code> ，这个是类型转换的函数，所以我们只需要把Session转换成它的包装类即可，即</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lisp">getOutputInfo((<span class="hljs-name">OrtSession*</span>) session)<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure><p><code>a nonstatic member reference must be relative to a specific object</code> ：函数不是静态函数，所以必须通过方法访问。</p><h3 id="Android"><a href="#Android" class="headerlink" title="Android"></a>Android</h3><p>安卓平台和Linux平台的实现基本上都是一样的，参考我的两个demo：<a href="https://github.com/BangwenHe/ORTSegDemo%E5%92%8Chttps://github.com/BangwenHe/ort-snake-cpp%E3%80%82%E4%B8%BB%E8%A6%81%E7%9A%84%E6%AD%A5%E9%AA%A4%E5%B0%B1%E6%98%AF%E7%BC%96%E5%86%99cmake%EF%BC%8C%E9%80%9A%E8%BF%87cmake%E6%8A%8A%E9%A2%84%E7%BC%96%E8%AF%91%E7%9A%84so%E5%BA%93%E9%93%BE%E6%8E%A5%E5%88%B0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E3%80%82%E5%AE%89%E5%8D%93%E4%B8%8A%E6%9B%B4%E9%9C%80%E8%A6%81%E8%80%83%E8%99%91%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86JNI%EF%BC%8C%E4%BB%A5%E5%8F%8A%E7%BC%96%E8%AF%91%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82">https://github.com/BangwenHe/ORTSegDemo和https://github.com/BangwenHe/ort-snake-cpp。主要的步骤就是编写cmake，通过cmake把预编译的so库链接到项目中。安卓上更需要考虑怎么处理JNI，以及编译的问题。</a></p><p>JNI的影响是jstring到string，long到jlong。</p><p>编译：由于java版本和c++版本都有<code>libonnxruntime.so</code> 这个文件，所以如果打包到一起，会导致文件重复而报错。</p><p>我写了一个 OnnxRuntime 手机端推理的 Demo，OrtSegDemo。只要能够导出 ONNX 文件，就可以用它在 CPU 上实现推理，GPU 不一定能够成功。下面是用法：</p><p><img src="https://s2.loli.net/2024/03/12/bAGS7psJmnuhz2x.png" alt="1280X1280.PNG"></p><p>需要修改的地方就是上面三个箭头指向的地方：</p><ol><li>把导出的 ONNX 模型放到 raw 文件夹中，注意文件名只能由字母、数字和下划线组成</li><li>添加一个新的 ResId 变量，例如导出的文件名叫做 e2ec_sim.onnx ，那么可以加上一条新的语句：int e2ecId &#x3D; R.raw.e2ec_sim; </li><li>修改第 54 行的传入参数，将 tinyposePath 修改成 e2ecId</li><li>编译运行整个项目</li></ol><p>下面是我的运行结果：</p><p><img src="https://s2.loli.net/2024/03/12/jrlUzmiu9F58vZS.png" alt="7f108cf0-91dd-43ca-943c-b1ef133b04b7.png"></p><p>可以看到，最后会显示一个平均延时。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MNN GPU float16 使用原理</title>
    <link href="/2024/03/11/MNN_GPU_float16/"/>
    <url>/2024/03/11/MNN_GPU_float16/</url>
    
    <content type="html"><![CDATA[<p>我观察到 MNN 在使用 GPU OpenCL 时，会默认使用 float16 的格式，导致模型评测时时间不同，如图。因此查看了 MNN 的源码，发现了一些有趣的东西。</p><p><img src="https://s2.loli.net/2024/03/11/MPzmpRb4dDFEO1K.png" alt="Snipaste_2024-03-11_16-44-19.png"></p><p>MNN 使用 <code>MNN::BackendConfig::Precision_Low</code> 时，会根据 GPU 的实际情况判断是否使用 float16 的数据格式。代码随附。</p><p>当导出的模型可以使用 float32 或者 float16 保存，当权重转换到 GPU 上时，会转换格式，在代码的第19行到第22行。代码随附。</p><p>Pipeline 中保存的 tensor 指向的 opencl buffer 保存的还是 float16。但是 <code>OpenCL::onMapTensor</code> 和 <code>OpenCL::onUnmapTensor</code> 的实现保证了映射前后得到的是 float32。<code>OpenCLBackend::onAcquire</code> 给出了 tensor 中保存的 buffer 格式，其中调用了 <code>isSupportedFP16</code> 判断目前是否支持 float16，如果支持则使用 float16 的大小创建 buffer。</p><p><code>OpenCLBackend::onMapTensor</code> 给出了映射 gpu buffer 到 cpu 上的实现。调用 <code>onMapTensor</code> 时如果不支持 SVM，会创建一个新的 cpu 内存块（<code>svmPtr = allocMapTensorMemory</code>），可以执行任意操作。对这块内存操作完毕后，如果是以 <code>MAP_TENSOR_WRITE</code> 的形式创建的 tensor，会将内存重新写回到 gpu buffer 中（<code>onCopyBuffer(&amp;srcTensor, dstTensor)</code>），这个时候会执行一次数据格式转换，包括 float16 到 float32。这表示：<strong>每次只能 map 一个 gpu tensor，不能 map 两个</strong>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// MNN OpenCLBackend.cpp</span><br><span class="hljs-function">Backend::MemObj* <span class="hljs-title">OpenCLBackend::onAcquire</span><span class="hljs-params">(<span class="hljs-type">const</span> Tensor* nativeTensor, StorageType storageType)</span> </span>&#123;<br>    <span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> LOG_VERBOSE</span><br>    <span class="hljs-built_in">MNN_PRINT</span>(<span class="hljs-string">&quot;Start OpenCLBackend::onAcquireBuffer !\n&quot;</span>);<br>    <span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-keyword">auto</span> tensorShape = OpenCL::<span class="hljs-built_in">tensorShapeFormat</span>(nativeTensor);<br>    <span class="hljs-type">int</span> N = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">0</span>);<br>    <span class="hljs-type">int</span> H = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-type">int</span> W = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">2</span>);<br>    <span class="hljs-type">int</span> C = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">3</span>);<br><br>    <span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> LOG_VERBOSE</span><br>    <span class="hljs-built_in">MNN_PRINT</span>(<span class="hljs-string">&quot;OpenCLBackend::onAcquireBuffer: NHWC:[%d, %d, %d, %d]\n&quot;</span>, N, H, W, C);<br>    <span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-meta">#<span class="hljs-keyword">ifndef</span> MNN_OPENCL_BUFFER_CLOSED</span><br>    <span class="hljs-keyword">if</span>(mOpenCLRuntime-&gt;<span class="hljs-built_in">getGpuMemType</span>() == BUFFER) &#123;<br>        <span class="hljs-type">size_t</span> size;<br>        <span class="hljs-keyword">if</span> (nativeTensor-&gt;<span class="hljs-built_in">dimensions</span>() &gt;= <span class="hljs-number">2</span>) &#123;<br>            <span class="hljs-keyword">auto</span> alignC = <span class="hljs-built_in">ROUND_UP</span>(C, <span class="hljs-number">8</span>);<br>            <span class="hljs-comment">// increment of height and width</span><br>            <span class="hljs-keyword">auto</span> hR = <span class="hljs-built_in">ROUND_UP</span>(H + <span class="hljs-number">3</span>, <span class="hljs-number">4</span>) - H;<br>            <span class="hljs-keyword">auto</span> wR = <span class="hljs-built_in">ROUND_UP</span>(W + <span class="hljs-number">3</span>, <span class="hljs-number">4</span>) - W;<br>            size = N * alignC * W * H;<br>            size = size + hR * W * <span class="hljs-number">4</span> + wR * <span class="hljs-number">4</span>;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            size = nativeTensor-&gt;<span class="hljs-built_in">elementSize</span>();<br>            size = <span class="hljs-built_in">ROUND_UP</span>(size, <span class="hljs-number">4</span>);<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> (mOpenCLRuntime-&gt;<span class="hljs-built_in">isSupportedIntelSubgroup</span>()) &#123;<br>            <span class="hljs-type">int</span> cPack = TensorUtils::<span class="hljs-built_in">getTensorChannelPack</span>(nativeTensor);<br>            <span class="hljs-keyword">auto</span> pads  = TensorUtils::<span class="hljs-built_in">getDescribe</span>(nativeTensor)-&gt;mPads;<br>            <span class="hljs-type">size_t</span> imageWidth  = (<span class="hljs-type">size_t</span>) <span class="hljs-built_in">ROUND_UP</span>(<span class="hljs-built_in">UP_DIV</span>(C, cPack), <span class="hljs-number">2</span>) * <span class="hljs-built_in">ROUND_UP</span>(pads.left + W + pads.right, <span class="hljs-number">4</span>);<span class="hljs-comment">//C-round to 8,W-round to 4, for memory alloc</span><br>            <span class="hljs-type">size_t</span> imageHeight = (<span class="hljs-type">size_t</span>)N * H;<br>            size = imageWidth*imageHeight*cPack;<br>        &#125;<br>        cl_channel_type dataType = CL_FLOAT;<br>        <span class="hljs-comment">//when support and want fp16, use half datatype</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">isSupportedFP16</span>()) &#123;<br>            dataType = CL_HALF_FLOAT;<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC_SEPERATE) &#123;<br>            <span class="hljs-keyword">auto</span> buffer = mBufferPool-&gt;<span class="hljs-built_in">alloc</span>(size*<br>                          (dataType==CL_HALF_FLOAT?<span class="hljs-built_in">sizeof</span>(half_float::half):<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)), <span class="hljs-literal">true</span>);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer;<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mBufferPool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC) &#123;<br>            <span class="hljs-keyword">auto</span> buffer = mBufferPool-&gt;<span class="hljs-built_in">alloc</span>(size*<br>                          (dataType==CL_HALF_FLOAT?<span class="hljs-built_in">sizeof</span>(half_float::half):<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)));<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer;<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mBufferPool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-built_in">MNN_ASSERT</span>(storageType == STATIC);<br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> MNN_LOW_MEMORY</span><br>        <span class="hljs-comment">// for weight quant model&#x27;s weight</span><br>        <span class="hljs-keyword">if</span> ((nativeTensor-&gt;<span class="hljs-built_in">getType</span>().code == halide_type_int) &amp;&amp;<br>            (nativeTensor-&gt;<span class="hljs-built_in">getType</span>().bits == <span class="hljs-number">8</span> || nativeTensor-&gt;<span class="hljs-built_in">getType</span>().bits == <span class="hljs-number">4</span>)) &#123;<br>            <span class="hljs-comment">// int8 quant</span><br>            <span class="hljs-type">size_t</span> alloc_size = size;<br>            <span class="hljs-keyword">if</span> (nativeTensor-&gt;<span class="hljs-built_in">getType</span>().bits == <span class="hljs-number">4</span>) &#123;<br>                <span class="hljs-comment">// int4 quant</span><br>                alloc_size = size / <span class="hljs-number">2</span>;<br>            &#125;<br>            <span class="hljs-keyword">auto</span> buffer = mStaticBufferPool-&gt;<span class="hljs-built_in">alloc</span>(alloc_size);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer;<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mStaticBufferPool.<span class="hljs-built_in">get</span>());<br>        &#125;<br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br>        <span class="hljs-keyword">auto</span> buffer = mStaticBufferPool-&gt;<span class="hljs-built_in">alloc</span>(size*<br>                     (dataType == CL_HALF_FLOAT ? <span class="hljs-built_in">sizeof</span>(half_float::half) : <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)));<br>        ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer; <span class="hljs-comment">// fix</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mStaticBufferPool.<span class="hljs-built_in">get</span>());<br>    &#125;<br>    <span class="hljs-keyword">else</span><br>    <span class="hljs-meta">#<span class="hljs-keyword">endif</span> <span class="hljs-comment">/* MNN_OPENCL_BUFFER_CLOSED */</span></span><br>    &#123;<br>        <span class="hljs-type">size_t</span> imageWidth  = (<span class="hljs-type">size_t</span>) (<span class="hljs-built_in">UP_DIV</span>(C, <span class="hljs-number">4</span>) * W);<span class="hljs-comment">//image mode only C pack to 4</span><br>        <span class="hljs-type">size_t</span> imageHeight = (<span class="hljs-type">size_t</span>)N * H;<br>        cl_channel_type dataType = CL_HALF_FLOAT;<br>        <span class="hljs-comment">//when user want high precision, use float datatype</span><br>        <span class="hljs-keyword">if</span> (mPrecision == BackendConfig::Precision_High) &#123;<br>            dataType = CL_FLOAT;<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC_SEPERATE) &#123;<br>            <span class="hljs-keyword">auto</span> image                               = mImagePool-&gt;<span class="hljs-built_in">alloc</span>(imageWidth, imageHeight, dataType, <span class="hljs-literal">true</span>);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)image; <span class="hljs-comment">// fix</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseImage</span>(image, mImagePool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC) &#123;<br>            <span class="hljs-keyword">auto</span> image                               = mImagePool-&gt;<span class="hljs-built_in">alloc</span>(imageWidth, imageHeight, dataType);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)image; <span class="hljs-comment">// fix</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseImage</span>(image, mImagePool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-built_in">MNN_ASSERT</span>(storageType == STATIC);<br>        <span class="hljs-keyword">auto</span> image                               = mStaticImagePool-&gt;<span class="hljs-built_in">alloc</span>(imageWidth, imageHeight, dataType);<br>        ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)image; <span class="hljs-comment">// fix</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseImage</span>(image, mStaticImagePool.<span class="hljs-built_in">get</span>());<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span>* <span class="hljs-title">OpenCLBackend::onMapTensor</span><span class="hljs-params">(Tensor::MapType mtype, Tensor::DimensionType dtype, <span class="hljs-type">const</span> Tensor* srcTensor)</span> </span>&#123;<br>    <span class="hljs-keyword">auto</span> needSize = srcTensor-&gt;<span class="hljs-built_in">size</span>();<br>    <span class="hljs-built_in">clearRecord</span>();<br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> MNN_OPENCL_SVM_ENABLE</span><br>    <span class="hljs-keyword">auto</span> svm_cap_ = mOpenCLRuntime-&gt;<span class="hljs-built_in">getSvmCapabilities</span>();<br>    <span class="hljs-type">bool</span> use_svm = (svm_cap_ &amp; CL_DEVICE_SVM_FINE_GRAIN_BUFFER);<span class="hljs-comment">//support fine grain svm</span><br>    use_svm |= ((svm_cap_ &amp; CL_DEVICE_SVM_COARSE_GRAIN_BUFFER) &amp;&amp; mOpenCLRuntime-&gt;<span class="hljs-built_in">getGpuType</span>() == ADRENO);<span class="hljs-comment">//support coarse grain svm and adreno gpu</span><br><br>    mUseSvm = (mOpenCLRuntime-&gt;<span class="hljs-built_in">getCLVersion</span>() &gt; <span class="hljs-number">1.99f</span> &amp;&amp; use_svm);<br>    <span class="hljs-keyword">if</span>(mUseSvm) &#123;<span class="hljs-comment">// CL version beyond 2.0 &amp; support svm</span><br>        svmPtr = <span class="hljs-built_in">allocMapTensorMemory</span>(needSize, <span class="hljs-literal">true</span>, svm_cap_);<br><br>        <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_READ) &#123;<br>            <span class="hljs-comment">//tmpTensor alloc</span><br>            <span class="hljs-function">MNN::Tensor <span class="hljs-title">tmpTensor</span><span class="hljs-params">(srcTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>            tmpTensor.<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)svmPtr;<br><br>            <span class="hljs-comment">//Convert format</span><br>            MNN_DATA_FORMAT format_type = MNN_DATA_FORMAT_NCHW;<br>            <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::TENSORFLOW) &#123;<br>                format_type = MNN_DATA_FORMAT_NHWC;<br>            &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::CAFFE_C4) &#123;<br>                format_type = MNN_DATA_FORMAT_NC4HW4;<br>            &#125;<br>            mCLRuntime-&gt;<span class="hljs-built_in">convertFromDevice</span>(srcTensor, &amp;tmpTensor, format_type, <span class="hljs-literal">true</span>);<br>        &#125;<br><br>        <span class="hljs-keyword">if</span>(svm_cap_ &amp; CL_DEVICE_SVM_FINE_GRAIN_BUFFER) &#123;<br>            <span class="hljs-comment">//Make sure command finished</span><br>            mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">finish</span>();<br>            <span class="hljs-keyword">return</span> svmPtr;<br>        &#125;<br><br>        <span class="hljs-keyword">auto</span> map_flag = CL_MAP_WRITE;<br>        <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_READ) &#123;<br>            map_flag = CL_MAP_READ;<br>        &#125;<br><br>        cl_int res = <span class="hljs-built_in">clEnqueueSVMMap</span>(mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">get</span>(), <span class="hljs-literal">true</span>, map_flag, svmPtr, needSize, <span class="hljs-number">0</span>, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>);<br><br>        <span class="hljs-built_in">MNN_CHECK_CL_SUCCESS</span>(res, <span class="hljs-string">&quot;svm_map&quot;</span>)<br>        <span class="hljs-keyword">return</span> svmPtr;<br>    &#125;<br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">    Not Support Svm, Use onopyBuffer</span><br><span class="hljs-comment">     */</span><br>    svmPtr = <span class="hljs-built_in">allocMapTensorMemory</span>(needSize, <span class="hljs-literal">false</span>);<br><br>    <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_READ) &#123;<br>        <span class="hljs-comment">//tmpTensor alloc</span><br>        <span class="hljs-function">MNN::Tensor <span class="hljs-title">tmpTensor</span><span class="hljs-params">(srcTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>        tmpTensor.<span class="hljs-built_in">buffer</span>().host = (<span class="hljs-type">uint8_t</span> *)svmPtr;<br><br>        <span class="hljs-comment">//use onCopyBuffer</span><br>        <span class="hljs-built_in">onCopyBuffer</span>(srcTensor, &amp;tmpTensor);<br>    &#125;<br>    <span class="hljs-keyword">return</span> svmPtr;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">OpenCLBackend::onUnmapTensor</span><span class="hljs-params">(Tensor::MapType mtype, Tensor::DimensionType dtype, <span class="hljs-type">const</span> Tensor* dstTensor, <span class="hljs-type">void</span>* mapPtr)</span> </span>&#123;<br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> MNN_OPENCL_SVM_ENABLE</span><br>    <span class="hljs-keyword">auto</span> svm_cap_ = mOpenCLRuntime-&gt;<span class="hljs-built_in">getSvmCapabilities</span>();<br>    <span class="hljs-keyword">if</span>(mUseSvm) &#123;<span class="hljs-comment">// CL version beyond 2.0 &amp; support svm</span><br><br>        <span class="hljs-comment">//If COARSE_SVM, Unmap first</span><br>        <span class="hljs-keyword">if</span>(!(svm_cap_ &amp; CL_DEVICE_SVM_FINE_GRAIN_BUFFER)) &#123;<br>            cl_int res = <span class="hljs-built_in">clEnqueueSVMUnmap</span>(mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">get</span>(), svmPtr, <span class="hljs-number">0</span>, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>);<br>            <span class="hljs-built_in">MNN_CHECK_CL_SUCCESS</span>(res, <span class="hljs-string">&quot;svm_unmap&quot;</span>)<br>        &#125;<br><br>        <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_WRITE) &#123;<br>            <span class="hljs-comment">//interTensor alloc</span><br>            <span class="hljs-function">MNN::Tensor <span class="hljs-title">interTensor</span><span class="hljs-params">(dstTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>            interTensor.<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)svmPtr;<br><br>            <span class="hljs-comment">//Convert format</span><br>            MNN_DATA_FORMAT format_type = MNN_DATA_FORMAT_NCHW;<br>            <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::TENSORFLOW) &#123;<br>                format_type = MNN_DATA_FORMAT_NHWC;<br>            &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::CAFFE_C4) &#123;<br>                format_type = MNN_DATA_FORMAT_NC4HW4;<br>            &#125;<br>            mCLRuntime-&gt;<span class="hljs-built_in">convertToDevice</span>(&amp;interTensor, dstTensor, format_type, <span class="hljs-literal">true</span>);<br>        &#125;<br>        mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">finish</span>();<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>    &#125;<br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">    Not Support Svm, Use onopyBuffer</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_WRITE) &#123;<br>        <span class="hljs-comment">//srcTensor alloc</span><br>        <span class="hljs-function">MNN::Tensor <span class="hljs-title">srcTensor</span><span class="hljs-params">(dstTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>        srcTensor.<span class="hljs-built_in">buffer</span>().host = (<span class="hljs-type">uint8_t</span> *)svmPtr;<br><br>        <span class="hljs-comment">//use onCopyBuffer</span><br>        <span class="hljs-built_in">onCopyBuffer</span>(&amp;srcTensor, dstTensor);<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>&#125;<br><br><br><span class="hljs-comment">// MNN ConvBufExecution.cpp</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">ConvBufExecution::setConv1x1WeightBuffer</span><span class="hljs-params">(<span class="hljs-type">int</span> packCout, <span class="hljs-type">int</span> packCin, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* filterDataPtr)</span> </span>&#123;<br>    cl_int res;<br>    <span class="hljs-function">std::shared_ptr&lt;Tensor&gt; <span class="hljs-title">filterBuffer</span><span class="hljs-params">(Tensor::createDevice&lt;<span class="hljs-type">float</span>&gt;(&#123;ROUND_UP(mOutputChannel, <span class="hljs-number">8</span>)<span class="hljs-comment">/*Cout pack set to max 8*/</span>, ROUND_UP(mInputChannel, packCin), mKernelWidth, mKernelHeight&#125;))</span></span>;<br>    <br>    <span class="hljs-type">int</span> buffer_size = filterBuffer-&gt;<span class="hljs-built_in">elementSize</span>();<br>    <span class="hljs-keyword">if</span>(mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">isSupportedFP16</span>()) &#123;<br>        buffer_size *= <span class="hljs-built_in">sizeof</span>(half_float::half);<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        buffer_size *= <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    &#125;<br>    mKernelBuffer.<span class="hljs-built_in">reset</span>(<span class="hljs-keyword">new</span> cl::<span class="hljs-built_in">Buffer</span>(mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">context</span>(), CL_MEM_READ_WRITE | CL_MEM_ALLOC_HOST_PTR, buffer_size));<br>    <span class="hljs-keyword">auto</span> kernelBufferPtr = mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">enqueueMapBuffer</span>(*(mKernelBuffer.<span class="hljs-built_in">get</span>()), <span class="hljs-literal">true</span>, CL_MAP_WRITE, <span class="hljs-number">0</span>, buffer_size, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>, &amp;res);<br>    <span class="hljs-keyword">if</span>(kernelBufferPtr != <span class="hljs-literal">nullptr</span> &amp;&amp; res == CL_SUCCESS)&#123;<br>        ::<span class="hljs-built_in">memset</span>(kernelBufferPtr, <span class="hljs-number">0</span>, buffer_size);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> o = <span class="hljs-number">0</span>; o &lt; mOutputChannel; o++)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span> ; i &lt; mInputChannel; i++)&#123;<br>                <span class="hljs-type">int</span> bufferIdx = (o/packCout) * <span class="hljs-built_in">ROUND_UP</span>(mInputChannel, packCin)*packCout + (i/packCin)*packCin*packCout + (o%packCout)*packCin + (i%packCin);<span class="hljs-comment">//(Co/packCout, Ci/packCin, packCout, packCin)</span><br>                <span class="hljs-type">int</span> filterIdx = o*mInputChannel + i;<br>                <span class="hljs-keyword">if</span>(mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">isSupportedFP16</span>())&#123;<br>                    ((half_float::half*)kernelBufferPtr)[bufferIdx] = (half_float::half)(filterDataPtr[filterIdx]);<br>                &#125;<span class="hljs-keyword">else</span>&#123;<br>                    ((<span class="hljs-type">float</span>*)kernelBufferPtr)[bufferIdx] = (<span class="hljs-type">float</span>)(filterDataPtr[filterIdx]);<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<span class="hljs-keyword">else</span>&#123;<br>        <span class="hljs-built_in">MNN_ERROR</span>(<span class="hljs-string">&quot;Map error ptrCL == nullptr \n&quot;</span>);<br>        <span class="hljs-built_in">MNN_ASSERT</span>(<span class="hljs-literal">false</span>);<br>    &#125;<br>    mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">enqueueUnmapMemObject</span>(*(mKernelBuffer.<span class="hljs-built_in">get</span>()), kernelBufferPtr);<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// MNN OpenCLRuntime.cpp</span><br>cl_device_fp_config fpConfig;<br><span class="hljs-keyword">auto</span> success = mFirstGPUDevicePtr-&gt;<span class="hljs-built_in">getInfo</span>(CL_DEVICE_HALF_FP_CONFIG, &amp;fpConfig);<br>mIsDeviceSupportedFP16     = CL_SUCCESS == success &amp;&amp; fpConfig &gt; <span class="hljs-number">0</span>;<br><br><span class="hljs-comment">//set gpu mode, tuning level and memory object</span><br><span class="hljs-built_in">setGpuMode</span>(cl_mode);<br><br><span class="hljs-keyword">if</span>(mMemType == AUTO) &#123;<br>    <span class="hljs-keyword">if</span>(mGpuType == MALI || mGpuType == INTEL) &#123;<br>        mMemType = BUFFER;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        mMemType = IMAGE;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">auto</span> permitFloat16 = <span class="hljs-literal">false</span>;<br><span class="hljs-keyword">if</span> (precision == BackendConfig::Precision_Low || (mMemType == BUFFER &amp;&amp; precision == BackendConfig::Precision_Normal)) &#123;<span class="hljs-comment">//buffer mode not support Normal Precision yet</span><br>    permitFloat16 = <span class="hljs-literal">true</span>;<br>&#125;<br>mIsSupportedFP16 = mIsDeviceSupportedFP16 &amp;&amp; permitFloat16;<br><span class="hljs-built_in">MNN_PRINT</span>(<span class="hljs-string">&quot;opencl support fp16: %d, device support fp16: %d, permit fp16: %d\n&quot;</span>, mIsSupportedFP16, mIsDeviceSupportedFP16, permitFloat16);<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/03/11/hello-world/"/>
    <url>/2024/03/11/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
