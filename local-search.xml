<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ORT 模型部署</title>
    <link href="/2024/03/12/Deploy-ORT-model/"/>
    <url>/2024/03/12/Deploy-ORT-model/</url>
    
    <content type="html"><![CDATA[<h1 id="Deploy-ORT-model"><a href="#Deploy-ORT-model" class="headerlink" title="Deploy ORT model"></a>Deploy ORT model</h1><p><a href="https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/notebooks/imagenet_v2/mobilenet.ipynb">onnxruntime-inference-examples&#x2F;mobilenet.ipynb at main · microsoft&#x2F;onnxruntime-inference-examples</a></p><p><a href="https://zhuanlan.zhihu.com/p/128974102">详细记录YOLACT实例分割ncnn实现</a></p><p>真正部署模型，不应该把后处理包括在模型推理中，这会影响模型在GPU上的部署，性能也不一定会好。这里的后处理，不仅仅是<code>model(input)</code> 之后的，也可以是作者放在模型推理过程中，但是实际上可以归为后处理的部分。</p><p>判断函数是否在ONNX trace的过程中：<code>torch.onnx.is_in_onnx_export()</code> 。</p><p><a href="https://www.notion.so/ONNX-to-TF-6049662a9e71472aaff545676a19050c">ONNX to TF</a></p><h2 id="Torch-to-ONNX"><a href="#Torch-to-ONNX" class="headerlink" title="Torch to ONNX"></a>Torch to ONNX</h2><ol><li>torch不支持<code>F.grid_sample</code> 算子。从<a href="https://github.com/onnx/onnx/blob/main/docs/Operators.md">ONNX支持的算子列表</a>来看，<code>opset=16</code> 时，可以直接使用<code>grid_sampler</code> 而不需要手动设置符号函数。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># https://github.com/pytorch/pytorch/issues/27212#issuecomment-1059773074</span><br><span class="hljs-comment"># https://gist.github.com/daigo0927/8c8b3005cffb61983e80ceab6c1f2274</span><br><span class="hljs-comment"># https://github.com/onnx/onnx/pull/3557</span><br><br><span class="hljs-keyword">from</span> torch.onnx <span class="hljs-keyword">import</span> register_custom_op_symbolic<br><span class="hljs-keyword">import</span> torch.onnx.symbolic_helper <span class="hljs-keyword">as</span> sym_help<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">grid_sampler</span>(<span class="hljs-params">g, <span class="hljs-built_in">input</span>, grid, mode, padding_mode, align_corners</span>):<br>    <span class="hljs-comment"># mode</span><br>    <span class="hljs-comment">#   &#x27;bilinear&#x27;      : onnx::Constant[value=&#123;0&#125;]</span><br>    <span class="hljs-comment">#   &#x27;nearest&#x27;       : onnx::Constant[value=&#123;1&#125;]</span><br>    <span class="hljs-comment">#   &#x27;bicubic&#x27;       : onnx::Constant[value=&#123;2&#125;]</span><br>    <span class="hljs-comment"># padding_mode</span><br>    <span class="hljs-comment">#   &#x27;zeros&#x27;         : onnx::Constant[value=&#123;0&#125;]</span><br>    <span class="hljs-comment">#   &#x27;border&#x27;        : onnx::Constant[value=&#123;1&#125;]</span><br>    <span class="hljs-comment">#   &#x27;reflection&#x27;    : onnx::Constant[value=&#123;2&#125;]</span><br>    mode = sym_help._maybe_get_const(mode, <span class="hljs-string">&quot;i&quot;</span>)<br>    padding_mode = sym_help._maybe_get_const(padding_mode, <span class="hljs-string">&quot;i&quot;</span>)<br>    mode_str = [<span class="hljs-string">&#x27;bilinear&#x27;</span>, <span class="hljs-string">&#x27;nearest&#x27;</span>, <span class="hljs-string">&#x27;bicubic&#x27;</span>][mode]<br>    padding_mode_str = [<span class="hljs-string">&#x27;zeros&#x27;</span>, <span class="hljs-string">&#x27;border&#x27;</span>, <span class="hljs-string">&#x27;reflection&#x27;</span>][padding_mode]<br>    align_corners = <span class="hljs-built_in">int</span>(sym_help._maybe_get_const(align_corners, <span class="hljs-string">&quot;b&quot;</span>))<br><br>    <span class="hljs-keyword">return</span> g.op(<span class="hljs-string">&quot;com.microsoft::GridSample&quot;</span>, <span class="hljs-built_in">input</span>, grid,<br>                mode_s=mode_str,<br>                padding_mode_s=padding_mode_str,<br>                align_corners_i=align_corners)<br>    <br>register_custom_op_symbolic(<span class="hljs-string">&#x27;::grid_sampler&#x27;</span>, grid_sampler, <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><ol start="2"><li><code>_DCNv2</code> 完全不受支持。它是用CUDA编译的，无论是自带的DCNv2还是mmcv的DCNv2。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@staticmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">symbolic</span>(<span class="hljs-params"></span><br><span class="hljs-params">    g, <span class="hljs-built_in">input</span>, offset, mask, weight, bias, stride, padding, dilation, deformable_groups</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-keyword">from</span> torch.nn.modules.utils <span class="hljs-keyword">import</span> _pair<br><br>    stride = _pair(stride)<br>    padding = _pair(padding)<br>    dilation = _pair(dilation)<br>    <span class="hljs-comment"># as of trt 7, the dcn operation will be translated again by modifying the onnx file</span><br>    <span class="hljs-comment"># so the exporting code is kept to resemble the forward()</span><br>    <span class="hljs-keyword">return</span> g.op(<br>        <span class="hljs-string">&quot;custom_domain::_DCNv2&quot;</span>,<br>        <span class="hljs-built_in">input</span>,<br>        offset,<br>        mask,<br>        weight,<br>        bias,<br>        stride_i=stride,<br>        padding_i=padding,<br>        dilation_i=dilation,<br>        deformable_groups_i=deformable_groups,<br>    )<br></code></pre></td></tr></table></figure><p>在这里可以注册一个符号函数(<code>symbolic_function</code>)来自己手动通过cpp实现DCNv2，但是我其实并不确定它能不能运行到mobile端。最上面导出NCNN的方法其实说过，可以跳过它，就是不用它，换到一个Conv层就行。在实现层面上，参考下面的模型变量key：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">5</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.weight&#x27;</span><br><span class="hljs-number">6</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.bias&#x27;</span><br><span class="hljs-number">7</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.conv_offset_mask.weight&#x27;</span><br><span class="hljs-number">8</span>:<span class="hljs-string">&#x27;dla.ida_up.proj_1.conv.conv_offset_mask.bias&#x27;</span><br></code></pre></td></tr></table></figure><p>它自带了一个conv，同时还有一个conv_offset_mask来学习stride的微小偏移量，这使得我们可以不使用后面的conv_offset_mask，只使用conv本身的权重来推理。不过这样会丢掉一些性能。下面这个用的是TensorRT推理，所以可以支持CUDA。DCNv2的实现中，其中一个就是参考了这个repo的。它这里面实现了自定义的算子，并且用符号函数把算子和onnx计算图联系到了一起。</p><p><a href="https://github.com/CaoWGG/TensorRT-CenterNet/blob/master/readme/ctdet2onnx.md">TensorRT-CenterNet&#x2F;ctdet2onnx.md at master · CaoWGG&#x2F;TensorRT-CenterNet</a></p><ol start="3"><li><code>Floating point exception (core dumped)</code> 。这个报错出现地非常地无厘头，因为它没有任何报错栈，只是单纯地显示它出错了。如果用torch1.9，会有更多的报错信息，虽然基本上也看不了。截图如下：</li></ol><p><img src="https://s2.loli.net/2024/03/12/tpTenzEYO7GFwB6.png" alt="raw_error.png"></p><p><a href="https://discuss.pytorch.org/t/dealing-with-floating-point-exceptions/51882/3">Dealing with floating point exceptions</a></p><p>通过<code>gdb --args python [finetune.py](http://finetune.py) $&#123;params&#125;</code> ，可以查看到更多的报错信息，截图如下：</p><p><img src="https://s2.loli.net/2024/03/12/oIicYZWQ9tR1Av7.png" alt="gdb_callstack.png"></p><h2 id="ONNX-to-ORT"><a href="#ONNX-to-ORT" class="headerlink" title="ONNX to ORT"></a>ONNX to ORT</h2><ol><li>除法算子导致张量除法的datatype不对应。<code>onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from /media/tclab/980Pro/users/bangwhe/e2ec/gcn.onnx failed:Type Error: Type parameter (T) of Optype (Div) bound to different types (tensor(double) and tensor(float) in node (Div_337).</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原代码</span><br>img_poly[..., <span class="hljs-number">0</span>] = img_poly[..., <span class="hljs-number">0</span>] / (w / <span class="hljs-number">2.</span>) - <span class="hljs-number">1</span><br>img_poly[..., <span class="hljs-number">1</span>] = img_poly[..., <span class="hljs-number">1</span>] / (h / <span class="hljs-number">2.</span>) - <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 修改后的代码，w先做除法</span><br>img_poly[..., <span class="hljs-number">0</span>] = img_poly[..., <span class="hljs-number">0</span>] / w * <span class="hljs-number">2.</span> - <span class="hljs-number">1</span><br>img_poly[..., <span class="hljs-number">1</span>] = img_poly[..., <span class="hljs-number">1</span>] / h * <span class="hljs-number">2.</span> - <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><ol start="2"><li>循环中的<code>==</code> 生成了bool，可能不受支持。<code>onnxruntime.capi.onnxruntime_pybind11_state.InvalidGraph: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from /media/tclab/980Pro/users/bangwhe/e2ec/gcn.onnx failed:This is an invalid model. Type Error: Type &#39;tensor(bool)&#39; of input parameter (763) of operator (ScatterND) in node (ScatterND_543) is invalid.</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 原代码</span><br>batch_size = cnn_feature.size(<span class="hljs-number">0</span>)<br>gcn_feature = torch.zeros([img_poly.size(<span class="hljs-number">0</span>), cnn_feature.size(<span class="hljs-number">1</span>), img_poly.size(<span class="hljs-number">1</span>)]).to(img_poly.device)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>    poly = img_poly[ind == i].unsqueeze(<span class="hljs-number">0</span>)<br>    feature = torch.nn.functional.grid_sample(cnn_feature[i:i+<span class="hljs-number">1</span>], poly)[<span class="hljs-number">0</span>].permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>    gcn_feature[ind == i] = feature<br><br><span class="hljs-comment"># 修改后的代码</span><br>gcn_feature[<span class="hljs-number">0</span>] = torch.nn.functional.grid_sample(cnn_feature[<span class="hljs-number">0</span>:<span class="hljs-number">1</span>], img_poly[<span class="hljs-number">0</span>:<span class="hljs-number">1</span>].unsqueeze(<span class="hljs-number">0</span>))[<span class="hljs-number">0</span>].permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><h2 id="ORT-NNAPI"><a href="#ORT-NNAPI" class="headerlink" title="ORT NNAPI"></a>ORT NNAPI</h2><p><a href="https://github.com/BangwenHe/ORTSegDemo">https://github.com/BangwenHe/ORTSegDemo</a></p><p><a href="https://github.com/microsoft/onnxruntime/blob/master/java/src/test/java/sample/ScoreMNIST.java">onnxruntime&#x2F;ScoreMNIST.java at master · microsoft&#x2F;onnxruntime</a></p><p><a href="https://github.com/microsoft/onnxruntime-inference-examples/blob/main/mobile/examples/image_classification/android/app/src/main/java/ai/onnxruntime/example/imageclassifier/ImageUtil.kt">onnxruntime-inference-examples&#x2F;ImageUtil.kt at main · microsoft&#x2F;onnxruntime-inference-examples</a></p><p>导出成onnx或者ort模型后，部署到手机上肯定是会支持CPU的，但是是否支持GPU还需要看模型的支持或者NNAPI的支持。增加CPU的线程数可以直接使用<code>mSessionOptions.setIntraOpNumThreads(4);</code> ，肯定是支持的。</p><ol><li>shape错误，应该是slice（切片）算子不受到NNAPI的支持。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">W/System.err: ai.onnxruntime.OrtException: Error code - ORT_FAIL - <br>message: model_builder.cc:<span class="hljs-number">374</span> RegisterModelOutputs shape_proto cannot be null <span class="hljs-keyword">for</span> output: <span class="hljs-number">217</span><br></code></pre></td></tr></table></figure><ol start="2"><li>我认为我导出的这个e2ec或者是snake不能运行到mobile端的GPU上应该是因为添加了太多的逻辑运算，而不仅仅是Conv、BN、ReLU这些算子，例如NonZero，还有一些多维取值算子，例如ScatterND、Gather等。这些逻辑算子和取值算子对于逻辑运算单元少的GPU来说，并行起来是非常困难的。所以优化的角度应该是把类似后处理的逻辑运算给提取出来，只保留大部分的backbone，手动从heatmap中提取关键点的位置信息，<a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/posenet_mobilenet_v1_100_257x257_multi_kpt_stripped.tflite">posenet</a>输出的就是heatmap，需要手动做提取。</li></ol><h2 id="Build-ONNXRuntime-From-Source"><a href="#Build-ONNXRuntime-From-Source" class="headerlink" title="Build ONNXRuntime From Source"></a>Build ONNXRuntime From Source</h2><p><a href="https://onnxruntime.ai/docs/build/android.html">Build for Android</a></p><p>要求cmake 3.18+，android SDK需要手动通过sdkmanager下载。加上<code>--build_java</code> 会很慢，所以先删掉。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> PATH=/mnt/tbdisk/bangwhe/experiments/cmake-3.20.6-linux-x86_64/bin/:<span class="hljs-variable">$PATH</span><br><br>./build.sh --android --build_nnapi\<br>--android_sdk_path /mnt/tbdisk/bangwhe/env/Android/platforms/android-29/ \<br>--android_ndk_path /mnt/tbdisk/bangwhe/env/android-ndk-r20b/<br></code></pre></td></tr></table></figure><p>编译得到的so库都保存在build文件夹中，下面有子文件夹，分别保存安卓端和Linux桌面端。</p><h2 id="OnnxRuntime-C-Impl"><a href="#OnnxRuntime-C-Impl" class="headerlink" title="OnnxRuntime C++ Impl"></a>OnnxRuntime C++ Impl</h2><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><p><a href="https://github.com/microsoft/onnxruntime-inference-examples/blob/main/c_cxx/model-explorer/model-explorer.cpp">onnxruntime-inference-examples&#x2F;model-explorer.cpp at main · microsoft&#x2F;onnxruntime-inference-examples</a></p><p><code>error: use of deleted function ‘Ort::Experimental::Session::Session(const Ort::Experimental::Session&amp;)</code> ：函数被删除了，后面有debug信息。这个session不能通过函数传到其它的函数里，从而被调用。<code>Ort::Value</code> 也是一样的，但是通过<code>std::move</code> 将其变成右值，就可以传到<code>vector</code> 中。（Why？）</p><p>vscode添加索引路径：command palette → C&#x2F;C++: Edit Configurations (JSON)</p><p><code>2022-08-15 10:50:20.544504982 [W:onnxruntime:, graph.cc:1220 Graph] Initializer 1894 appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.</code> initializer 1894 出现在图形输入中，不会被视为常量值&#x2F;权重，可能会阻止一些类似常量折叠的优化，可以使用<code>onnxruntime/tools/python/remove_initializer_from_input.py</code> 来进行移除操作。移除前的推理时间是45ms，移除后是19ms，提升还是比较大的。</p><p><code>cv:dnn:blobFromImage</code> 不太好用，经常把320x320的Mat变成3x1的Mat。（Why？）</p><p>OpenCV的<code>Mat</code> 类和ORT的<code>Ort::Value</code> 类都可以得到它们的数据指针，方法分别是：</p><ul><li><code>[Mat.data](http://Mat.data)</code> 可以直接得到数据的指针，但是更常用的是<code>Mat.at&lt;DataType&gt;()</code></li><li><code>Ort::Value::GetTensorMutableData&lt;DataType&gt;()</code> ，得到<code>Ort::Value</code> 保存的张量的数据指针</li><li>通过这两个指针，我们可以得到预处理得到的结果和输入网络前的张量的大小。如果需要对比预处理是否相同，可以自己读指针，然后打印数据出来跟python版的数据做对比。</li></ul><p><code>terminate called after throwing an instance of &#39;Ort::Exception&#39; what():  not enough space: expected 1228800, got 409600</code> ：明显的报错，空间不够。这是因为传入的<code>p_data_element_count</code> 和<code>*shape</code> 的乘积大小不相同，所以导致了报错，传入的参数为<code>102400</code> 和<code>307200</code> ，再乘上float类型占用的4byte，有<code>409600</code> 和<code>1228800</code> ，所以商（3）代表少了三个通道，给<code>p_data_element_count</code> 乘上3即可。</p><p><code>incomplete type is not allowed</code> ：需要先定义一个type，才能在后续使用这个标识符的时候知道它是啥。这里报错是因为OrtSession这个类（标识符）不存在，需要变成指针类型才可以，即OrtSession*类型。</p><p><a href="https://www.notion.so/02-32a4f1797c1b46758f2a01ed714970bf">02</a></p><p><code>error: use of deleted function ‘Ort::Session::Session(const Ort::Session&amp;)’</code> ：使用了被删除的拷贝构造函数，所以不能把它当作变量传到某一个函数中，编译器就会报错（为什么会删除呢？参考我的02）。那我们总不可能不用函数吧？在<a href="https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/session/onnxruntime_c_api.h#L3500">3501</a>行，注释中说：</p><blockquote><p>Prevent users from accidentally copying the API structure, it should always be passed as a pointer.</p></blockquote><p>所以我们应该传入一个指针。从类名往上溯源，发现一个<code>Base</code> 类，它有一个函数：<code>operator T*() &#123; return p_; &#125;</code> ，这个是类型转换的函数，所以我们只需要把Session转换成它的包装类即可，即</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lisp">getOutputInfo((<span class="hljs-name">OrtSession*</span>) session)<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure><p><code>a nonstatic member reference must be relative to a specific object</code> ：函数不是静态函数，所以必须通过方法访问。</p><h3 id="Android"><a href="#Android" class="headerlink" title="Android"></a>Android</h3><p>安卓平台和Linux平台的实现基本上都是一样的，参考我的两个demo：<a href="https://github.com/BangwenHe/ORTSegDemo%E5%92%8Chttps://github.com/BangwenHe/ort-snake-cpp%E3%80%82%E4%B8%BB%E8%A6%81%E7%9A%84%E6%AD%A5%E9%AA%A4%E5%B0%B1%E6%98%AF%E7%BC%96%E5%86%99cmake%EF%BC%8C%E9%80%9A%E8%BF%87cmake%E6%8A%8A%E9%A2%84%E7%BC%96%E8%AF%91%E7%9A%84so%E5%BA%93%E9%93%BE%E6%8E%A5%E5%88%B0%E9%A1%B9%E7%9B%AE%E4%B8%AD%E3%80%82%E5%AE%89%E5%8D%93%E4%B8%8A%E6%9B%B4%E9%9C%80%E8%A6%81%E8%80%83%E8%99%91%E6%80%8E%E4%B9%88%E5%A4%84%E7%90%86JNI%EF%BC%8C%E4%BB%A5%E5%8F%8A%E7%BC%96%E8%AF%91%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82">https://github.com/BangwenHe/ORTSegDemo和https://github.com/BangwenHe/ort-snake-cpp。主要的步骤就是编写cmake，通过cmake把预编译的so库链接到项目中。安卓上更需要考虑怎么处理JNI，以及编译的问题。</a></p><p>JNI的影响是jstring到string，long到jlong。</p><p>编译：由于java版本和c++版本都有<code>libonnxruntime.so</code> 这个文件，所以如果打包到一起，会导致文件重复而报错。</p><p>我写了一个 OnnxRuntime 手机端推理的 Demo，OrtSegDemo。只要能够导出 ONNX 文件，就可以用它在 CPU 上实现推理，GPU 不一定能够成功。下面是用法：</p><p><img src="https://s2.loli.net/2024/03/12/bAGS7psJmnuhz2x.png" alt="1280X1280.PNG"></p><p>需要修改的地方就是上面三个箭头指向的地方：</p><ol><li>把导出的 ONNX 模型放到 raw 文件夹中，注意文件名只能由字母、数字和下划线组成</li><li>添加一个新的 ResId 变量，例如导出的文件名叫做 e2ec_sim.onnx ，那么可以加上一条新的语句：int e2ecId &#x3D; R.raw.e2ec_sim; </li><li>修改第 54 行的传入参数，将 tinyposePath 修改成 e2ecId</li><li>编译运行整个项目</li></ol><p>下面是我的运行结果：</p><p><img src="https://s2.loli.net/2024/03/12/jrlUzmiu9F58vZS.png" alt="7f108cf0-91dd-43ca-943c-b1ef133b04b7.png"></p><p>可以看到，最后会显示一个平均延时。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>MNN GPU float16 使用原理</title>
    <link href="/2024/03/11/MNN_GPU_float16/"/>
    <url>/2024/03/11/MNN_GPU_float16/</url>
    
    <content type="html"><![CDATA[<p>我观察到 MNN 在使用 GPU OpenCL 时，会默认使用 float16 的格式，导致模型评测时时间不同，如图。因此查看了 MNN 的源码，发现了一些有趣的东西。</p><p><img src="https://s2.loli.net/2024/03/11/MPzmpRb4dDFEO1K.png" alt="Snipaste_2024-03-11_16-44-19.png"></p><p>MNN 使用 <code>MNN::BackendConfig::Precision_Low</code> 时，会根据 GPU 的实际情况判断是否使用 float16 的数据格式。代码随附。</p><p>当导出的模型可以使用 float32 或者 float16 保存，当权重转换到 GPU 上时，会转换格式，在代码的第19行到第22行。代码随附。</p><p>Pipeline 中保存的 tensor 指向的 opencl buffer 保存的还是 float16。但是 <code>OpenCL::onMapTensor</code> 和 <code>OpenCL::onUnmapTensor</code> 的实现保证了映射前后得到的是 float32。<code>OpenCLBackend::onAcquire</code> 给出了 tensor 中保存的 buffer 格式，其中调用了 <code>isSupportedFP16</code> 判断目前是否支持 float16，如果支持则使用 float16 的大小创建 buffer。</p><p><code>OpenCLBackend::onMapTensor</code> 给出了映射 gpu buffer 到 cpu 上的实现。调用 <code>onMapTensor</code> 时如果不支持 SVM，会创建一个新的 cpu 内存块（<code>svmPtr = allocMapTensorMemory</code>），可以执行任意操作。对这块内存操作完毕后，如果是以 <code>MAP_TENSOR_WRITE</code> 的形式创建的 tensor，会将内存重新写回到 gpu buffer 中（<code>onCopyBuffer(&amp;srcTensor, dstTensor)</code>），这个时候会执行一次数据格式转换，包括 float16 到 float32。这表示：<strong>每次只能 map 一个 gpu tensor，不能 map 两个</strong>。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// MNN OpenCLBackend.cpp</span><br><span class="hljs-function">Backend::MemObj* <span class="hljs-title">OpenCLBackend::onAcquire</span><span class="hljs-params">(<span class="hljs-type">const</span> Tensor* nativeTensor, StorageType storageType)</span> </span>&#123;<br>    <span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> LOG_VERBOSE</span><br>    <span class="hljs-built_in">MNN_PRINT</span>(<span class="hljs-string">&quot;Start OpenCLBackend::onAcquireBuffer !\n&quot;</span>);<br>    <span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-keyword">auto</span> tensorShape = OpenCL::<span class="hljs-built_in">tensorShapeFormat</span>(nativeTensor);<br>    <span class="hljs-type">int</span> N = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">0</span>);<br>    <span class="hljs-type">int</span> H = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">1</span>);<br>    <span class="hljs-type">int</span> W = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">2</span>);<br>    <span class="hljs-type">int</span> C = tensorShape.<span class="hljs-built_in">at</span>(<span class="hljs-number">3</span>);<br><br>    <span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> LOG_VERBOSE</span><br>    <span class="hljs-built_in">MNN_PRINT</span>(<span class="hljs-string">&quot;OpenCLBackend::onAcquireBuffer: NHWC:[%d, %d, %d, %d]\n&quot;</span>, N, H, W, C);<br>    <span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-meta">#<span class="hljs-keyword">ifndef</span> MNN_OPENCL_BUFFER_CLOSED</span><br>    <span class="hljs-keyword">if</span>(mOpenCLRuntime-&gt;<span class="hljs-built_in">getGpuMemType</span>() == BUFFER) &#123;<br>        <span class="hljs-type">size_t</span> size;<br>        <span class="hljs-keyword">if</span> (nativeTensor-&gt;<span class="hljs-built_in">dimensions</span>() &gt;= <span class="hljs-number">2</span>) &#123;<br>            <span class="hljs-keyword">auto</span> alignC = <span class="hljs-built_in">ROUND_UP</span>(C, <span class="hljs-number">8</span>);<br>            <span class="hljs-comment">// increment of height and width</span><br>            <span class="hljs-keyword">auto</span> hR = <span class="hljs-built_in">ROUND_UP</span>(H + <span class="hljs-number">3</span>, <span class="hljs-number">4</span>) - H;<br>            <span class="hljs-keyword">auto</span> wR = <span class="hljs-built_in">ROUND_UP</span>(W + <span class="hljs-number">3</span>, <span class="hljs-number">4</span>) - W;<br>            size = N * alignC * W * H;<br>            size = size + hR * W * <span class="hljs-number">4</span> + wR * <span class="hljs-number">4</span>;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            size = nativeTensor-&gt;<span class="hljs-built_in">elementSize</span>();<br>            size = <span class="hljs-built_in">ROUND_UP</span>(size, <span class="hljs-number">4</span>);<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> (mOpenCLRuntime-&gt;<span class="hljs-built_in">isSupportedIntelSubgroup</span>()) &#123;<br>            <span class="hljs-type">int</span> cPack = TensorUtils::<span class="hljs-built_in">getTensorChannelPack</span>(nativeTensor);<br>            <span class="hljs-keyword">auto</span> pads  = TensorUtils::<span class="hljs-built_in">getDescribe</span>(nativeTensor)-&gt;mPads;<br>            <span class="hljs-type">size_t</span> imageWidth  = (<span class="hljs-type">size_t</span>) <span class="hljs-built_in">ROUND_UP</span>(<span class="hljs-built_in">UP_DIV</span>(C, cPack), <span class="hljs-number">2</span>) * <span class="hljs-built_in">ROUND_UP</span>(pads.left + W + pads.right, <span class="hljs-number">4</span>);<span class="hljs-comment">//C-round to 8,W-round to 4, for memory alloc</span><br>            <span class="hljs-type">size_t</span> imageHeight = (<span class="hljs-type">size_t</span>)N * H;<br>            size = imageWidth*imageHeight*cPack;<br>        &#125;<br>        cl_channel_type dataType = CL_FLOAT;<br>        <span class="hljs-comment">//when support and want fp16, use half datatype</span><br>        <span class="hljs-keyword">if</span> (<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">isSupportedFP16</span>()) &#123;<br>            dataType = CL_HALF_FLOAT;<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC_SEPERATE) &#123;<br>            <span class="hljs-keyword">auto</span> buffer = mBufferPool-&gt;<span class="hljs-built_in">alloc</span>(size*<br>                          (dataType==CL_HALF_FLOAT?<span class="hljs-built_in">sizeof</span>(half_float::half):<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)), <span class="hljs-literal">true</span>);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer;<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mBufferPool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC) &#123;<br>            <span class="hljs-keyword">auto</span> buffer = mBufferPool-&gt;<span class="hljs-built_in">alloc</span>(size*<br>                          (dataType==CL_HALF_FLOAT?<span class="hljs-built_in">sizeof</span>(half_float::half):<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)));<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer;<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mBufferPool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-built_in">MNN_ASSERT</span>(storageType == STATIC);<br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> MNN_LOW_MEMORY</span><br>        <span class="hljs-comment">// for weight quant model&#x27;s weight</span><br>        <span class="hljs-keyword">if</span> ((nativeTensor-&gt;<span class="hljs-built_in">getType</span>().code == halide_type_int) &amp;&amp;<br>            (nativeTensor-&gt;<span class="hljs-built_in">getType</span>().bits == <span class="hljs-number">8</span> || nativeTensor-&gt;<span class="hljs-built_in">getType</span>().bits == <span class="hljs-number">4</span>)) &#123;<br>            <span class="hljs-comment">// int8 quant</span><br>            <span class="hljs-type">size_t</span> alloc_size = size;<br>            <span class="hljs-keyword">if</span> (nativeTensor-&gt;<span class="hljs-built_in">getType</span>().bits == <span class="hljs-number">4</span>) &#123;<br>                <span class="hljs-comment">// int4 quant</span><br>                alloc_size = size / <span class="hljs-number">2</span>;<br>            &#125;<br>            <span class="hljs-keyword">auto</span> buffer = mStaticBufferPool-&gt;<span class="hljs-built_in">alloc</span>(alloc_size);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer;<br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mStaticBufferPool.<span class="hljs-built_in">get</span>());<br>        &#125;<br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br>        <span class="hljs-keyword">auto</span> buffer = mStaticBufferPool-&gt;<span class="hljs-built_in">alloc</span>(size*<br>                     (dataType == CL_HALF_FLOAT ? <span class="hljs-built_in">sizeof</span>(half_float::half) : <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)));<br>        ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)buffer; <span class="hljs-comment">// fix</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseBuffer</span>(buffer, mStaticBufferPool.<span class="hljs-built_in">get</span>());<br>    &#125;<br>    <span class="hljs-keyword">else</span><br>    <span class="hljs-meta">#<span class="hljs-keyword">endif</span> <span class="hljs-comment">/* MNN_OPENCL_BUFFER_CLOSED */</span></span><br>    &#123;<br>        <span class="hljs-type">size_t</span> imageWidth  = (<span class="hljs-type">size_t</span>) (<span class="hljs-built_in">UP_DIV</span>(C, <span class="hljs-number">4</span>) * W);<span class="hljs-comment">//image mode only C pack to 4</span><br>        <span class="hljs-type">size_t</span> imageHeight = (<span class="hljs-type">size_t</span>)N * H;<br>        cl_channel_type dataType = CL_HALF_FLOAT;<br>        <span class="hljs-comment">//when user want high precision, use float datatype</span><br>        <span class="hljs-keyword">if</span> (mPrecision == BackendConfig::Precision_High) &#123;<br>            dataType = CL_FLOAT;<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC_SEPERATE) &#123;<br>            <span class="hljs-keyword">auto</span> image                               = mImagePool-&gt;<span class="hljs-built_in">alloc</span>(imageWidth, imageHeight, dataType, <span class="hljs-literal">true</span>);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)image; <span class="hljs-comment">// fix</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseImage</span>(image, mImagePool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-keyword">if</span> (storageType == DYNAMIC) &#123;<br>            <span class="hljs-keyword">auto</span> image                               = mImagePool-&gt;<span class="hljs-built_in">alloc</span>(imageWidth, imageHeight, dataType);<br>            ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)image; <span class="hljs-comment">// fix</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseImage</span>(image, mImagePool.<span class="hljs-built_in">get</span>());<br>        &#125;<br>        <span class="hljs-built_in">MNN_ASSERT</span>(storageType == STATIC);<br>        <span class="hljs-keyword">auto</span> image                               = mStaticImagePool-&gt;<span class="hljs-built_in">alloc</span>(imageWidth, imageHeight, dataType);<br>        ((Tensor*)nativeTensor)-&gt;<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)image; <span class="hljs-comment">// fix</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-built_in">CLMemReleaseImage</span>(image, mStaticImagePool.<span class="hljs-built_in">get</span>());<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span>* <span class="hljs-title">OpenCLBackend::onMapTensor</span><span class="hljs-params">(Tensor::MapType mtype, Tensor::DimensionType dtype, <span class="hljs-type">const</span> Tensor* srcTensor)</span> </span>&#123;<br>    <span class="hljs-keyword">auto</span> needSize = srcTensor-&gt;<span class="hljs-built_in">size</span>();<br>    <span class="hljs-built_in">clearRecord</span>();<br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> MNN_OPENCL_SVM_ENABLE</span><br>    <span class="hljs-keyword">auto</span> svm_cap_ = mOpenCLRuntime-&gt;<span class="hljs-built_in">getSvmCapabilities</span>();<br>    <span class="hljs-type">bool</span> use_svm = (svm_cap_ &amp; CL_DEVICE_SVM_FINE_GRAIN_BUFFER);<span class="hljs-comment">//support fine grain svm</span><br>    use_svm |= ((svm_cap_ &amp; CL_DEVICE_SVM_COARSE_GRAIN_BUFFER) &amp;&amp; mOpenCLRuntime-&gt;<span class="hljs-built_in">getGpuType</span>() == ADRENO);<span class="hljs-comment">//support coarse grain svm and adreno gpu</span><br><br>    mUseSvm = (mOpenCLRuntime-&gt;<span class="hljs-built_in">getCLVersion</span>() &gt; <span class="hljs-number">1.99f</span> &amp;&amp; use_svm);<br>    <span class="hljs-keyword">if</span>(mUseSvm) &#123;<span class="hljs-comment">// CL version beyond 2.0 &amp; support svm</span><br>        svmPtr = <span class="hljs-built_in">allocMapTensorMemory</span>(needSize, <span class="hljs-literal">true</span>, svm_cap_);<br><br>        <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_READ) &#123;<br>            <span class="hljs-comment">//tmpTensor alloc</span><br>            <span class="hljs-function">MNN::Tensor <span class="hljs-title">tmpTensor</span><span class="hljs-params">(srcTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>            tmpTensor.<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)svmPtr;<br><br>            <span class="hljs-comment">//Convert format</span><br>            MNN_DATA_FORMAT format_type = MNN_DATA_FORMAT_NCHW;<br>            <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::TENSORFLOW) &#123;<br>                format_type = MNN_DATA_FORMAT_NHWC;<br>            &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::CAFFE_C4) &#123;<br>                format_type = MNN_DATA_FORMAT_NC4HW4;<br>            &#125;<br>            mCLRuntime-&gt;<span class="hljs-built_in">convertFromDevice</span>(srcTensor, &amp;tmpTensor, format_type, <span class="hljs-literal">true</span>);<br>        &#125;<br><br>        <span class="hljs-keyword">if</span>(svm_cap_ &amp; CL_DEVICE_SVM_FINE_GRAIN_BUFFER) &#123;<br>            <span class="hljs-comment">//Make sure command finished</span><br>            mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">finish</span>();<br>            <span class="hljs-keyword">return</span> svmPtr;<br>        &#125;<br><br>        <span class="hljs-keyword">auto</span> map_flag = CL_MAP_WRITE;<br>        <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_READ) &#123;<br>            map_flag = CL_MAP_READ;<br>        &#125;<br><br>        cl_int res = <span class="hljs-built_in">clEnqueueSVMMap</span>(mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">get</span>(), <span class="hljs-literal">true</span>, map_flag, svmPtr, needSize, <span class="hljs-number">0</span>, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>);<br><br>        <span class="hljs-built_in">MNN_CHECK_CL_SUCCESS</span>(res, <span class="hljs-string">&quot;svm_map&quot;</span>)<br>        <span class="hljs-keyword">return</span> svmPtr;<br>    &#125;<br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">    Not Support Svm, Use onopyBuffer</span><br><span class="hljs-comment">     */</span><br>    svmPtr = <span class="hljs-built_in">allocMapTensorMemory</span>(needSize, <span class="hljs-literal">false</span>);<br><br>    <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_READ) &#123;<br>        <span class="hljs-comment">//tmpTensor alloc</span><br>        <span class="hljs-function">MNN::Tensor <span class="hljs-title">tmpTensor</span><span class="hljs-params">(srcTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>        tmpTensor.<span class="hljs-built_in">buffer</span>().host = (<span class="hljs-type">uint8_t</span> *)svmPtr;<br><br>        <span class="hljs-comment">//use onCopyBuffer</span><br>        <span class="hljs-built_in">onCopyBuffer</span>(srcTensor, &amp;tmpTensor);<br>    &#125;<br>    <span class="hljs-keyword">return</span> svmPtr;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">OpenCLBackend::onUnmapTensor</span><span class="hljs-params">(Tensor::MapType mtype, Tensor::DimensionType dtype, <span class="hljs-type">const</span> Tensor* dstTensor, <span class="hljs-type">void</span>* mapPtr)</span> </span>&#123;<br><span class="hljs-meta">#<span class="hljs-keyword">ifdef</span> MNN_OPENCL_SVM_ENABLE</span><br>    <span class="hljs-keyword">auto</span> svm_cap_ = mOpenCLRuntime-&gt;<span class="hljs-built_in">getSvmCapabilities</span>();<br>    <span class="hljs-keyword">if</span>(mUseSvm) &#123;<span class="hljs-comment">// CL version beyond 2.0 &amp; support svm</span><br><br>        <span class="hljs-comment">//If COARSE_SVM, Unmap first</span><br>        <span class="hljs-keyword">if</span>(!(svm_cap_ &amp; CL_DEVICE_SVM_FINE_GRAIN_BUFFER)) &#123;<br>            cl_int res = <span class="hljs-built_in">clEnqueueSVMUnmap</span>(mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">get</span>(), svmPtr, <span class="hljs-number">0</span>, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>);<br>            <span class="hljs-built_in">MNN_CHECK_CL_SUCCESS</span>(res, <span class="hljs-string">&quot;svm_unmap&quot;</span>)<br>        &#125;<br><br>        <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_WRITE) &#123;<br>            <span class="hljs-comment">//interTensor alloc</span><br>            <span class="hljs-function">MNN::Tensor <span class="hljs-title">interTensor</span><span class="hljs-params">(dstTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>            interTensor.<span class="hljs-built_in">buffer</span>().device = (<span class="hljs-type">uint64_t</span>)svmPtr;<br><br>            <span class="hljs-comment">//Convert format</span><br>            MNN_DATA_FORMAT format_type = MNN_DATA_FORMAT_NCHW;<br>            <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::TENSORFLOW) &#123;<br>                format_type = MNN_DATA_FORMAT_NHWC;<br>            &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span>(dtype == MNN::Tensor::CAFFE_C4) &#123;<br>                format_type = MNN_DATA_FORMAT_NC4HW4;<br>            &#125;<br>            mCLRuntime-&gt;<span class="hljs-built_in">convertToDevice</span>(&amp;interTensor, dstTensor, format_type, <span class="hljs-literal">true</span>);<br>        &#125;<br>        mOpenCLRuntime-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">finish</span>();<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>    &#125;<br><span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br><br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">    Not Support Svm, Use onopyBuffer</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">if</span>(mtype == Tensor::MAP_TENSOR_WRITE) &#123;<br>        <span class="hljs-comment">//srcTensor alloc</span><br>        <span class="hljs-function">MNN::Tensor <span class="hljs-title">srcTensor</span><span class="hljs-params">(dstTensor, dtype, <span class="hljs-literal">false</span>)</span></span>;<br>        srcTensor.<span class="hljs-built_in">buffer</span>().host = (<span class="hljs-type">uint8_t</span> *)svmPtr;<br><br>        <span class="hljs-comment">//use onCopyBuffer</span><br>        <span class="hljs-built_in">onCopyBuffer</span>(&amp;srcTensor, dstTensor);<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>&#125;<br><br><br><span class="hljs-comment">// MNN ConvBufExecution.cpp</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">ConvBufExecution::setConv1x1WeightBuffer</span><span class="hljs-params">(<span class="hljs-type">int</span> packCout, <span class="hljs-type">int</span> packCin, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* filterDataPtr)</span> </span>&#123;<br>    cl_int res;<br>    <span class="hljs-function">std::shared_ptr&lt;Tensor&gt; <span class="hljs-title">filterBuffer</span><span class="hljs-params">(Tensor::createDevice&lt;<span class="hljs-type">float</span>&gt;(&#123;ROUND_UP(mOutputChannel, <span class="hljs-number">8</span>)<span class="hljs-comment">/*Cout pack set to max 8*/</span>, ROUND_UP(mInputChannel, packCin), mKernelWidth, mKernelHeight&#125;))</span></span>;<br>    <br>    <span class="hljs-type">int</span> buffer_size = filterBuffer-&gt;<span class="hljs-built_in">elementSize</span>();<br>    <span class="hljs-keyword">if</span>(mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">isSupportedFP16</span>()) &#123;<br>        buffer_size *= <span class="hljs-built_in">sizeof</span>(half_float::half);<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        buffer_size *= <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    &#125;<br>    mKernelBuffer.<span class="hljs-built_in">reset</span>(<span class="hljs-keyword">new</span> cl::<span class="hljs-built_in">Buffer</span>(mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">context</span>(), CL_MEM_READ_WRITE | CL_MEM_ALLOC_HOST_PTR, buffer_size));<br>    <span class="hljs-keyword">auto</span> kernelBufferPtr = mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">enqueueMapBuffer</span>(*(mKernelBuffer.<span class="hljs-built_in">get</span>()), <span class="hljs-literal">true</span>, CL_MAP_WRITE, <span class="hljs-number">0</span>, buffer_size, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>, &amp;res);<br>    <span class="hljs-keyword">if</span>(kernelBufferPtr != <span class="hljs-literal">nullptr</span> &amp;&amp; res == CL_SUCCESS)&#123;<br>        ::<span class="hljs-built_in">memset</span>(kernelBufferPtr, <span class="hljs-number">0</span>, buffer_size);<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> o = <span class="hljs-number">0</span>; o &lt; mOutputChannel; o++)&#123;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span> ; i &lt; mInputChannel; i++)&#123;<br>                <span class="hljs-type">int</span> bufferIdx = (o/packCout) * <span class="hljs-built_in">ROUND_UP</span>(mInputChannel, packCin)*packCout + (i/packCin)*packCin*packCout + (o%packCout)*packCin + (i%packCin);<span class="hljs-comment">//(Co/packCout, Ci/packCin, packCout, packCin)</span><br>                <span class="hljs-type">int</span> filterIdx = o*mInputChannel + i;<br>                <span class="hljs-keyword">if</span>(mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">isSupportedFP16</span>())&#123;<br>                    ((half_float::half*)kernelBufferPtr)[bufferIdx] = (half_float::half)(filterDataPtr[filterIdx]);<br>                &#125;<span class="hljs-keyword">else</span>&#123;<br>                    ((<span class="hljs-type">float</span>*)kernelBufferPtr)[bufferIdx] = (<span class="hljs-type">float</span>)(filterDataPtr[filterIdx]);<br>                &#125;<br>            &#125;<br>        &#125;<br>    &#125;<span class="hljs-keyword">else</span>&#123;<br>        <span class="hljs-built_in">MNN_ERROR</span>(<span class="hljs-string">&quot;Map error ptrCL == nullptr \n&quot;</span>);<br>        <span class="hljs-built_in">MNN_ASSERT</span>(<span class="hljs-literal">false</span>);<br>    &#125;<br>    mOpenCLBackend-&gt;<span class="hljs-built_in">getOpenCLRuntime</span>()-&gt;<span class="hljs-built_in">commandQueue</span>().<span class="hljs-built_in">enqueueUnmapMemObject</span>(*(mKernelBuffer.<span class="hljs-built_in">get</span>()), kernelBufferPtr);<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// MNN OpenCLRuntime.cpp</span><br>cl_device_fp_config fpConfig;<br><span class="hljs-keyword">auto</span> success = mFirstGPUDevicePtr-&gt;<span class="hljs-built_in">getInfo</span>(CL_DEVICE_HALF_FP_CONFIG, &amp;fpConfig);<br>mIsDeviceSupportedFP16     = CL_SUCCESS == success &amp;&amp; fpConfig &gt; <span class="hljs-number">0</span>;<br><br><span class="hljs-comment">//set gpu mode, tuning level and memory object</span><br><span class="hljs-built_in">setGpuMode</span>(cl_mode);<br><br><span class="hljs-keyword">if</span>(mMemType == AUTO) &#123;<br>    <span class="hljs-keyword">if</span>(mGpuType == MALI || mGpuType == INTEL) &#123;<br>        mMemType = BUFFER;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        mMemType = IMAGE;<br>    &#125;<br>&#125;<br><br><span class="hljs-keyword">auto</span> permitFloat16 = <span class="hljs-literal">false</span>;<br><span class="hljs-keyword">if</span> (precision == BackendConfig::Precision_Low || (mMemType == BUFFER &amp;&amp; precision == BackendConfig::Precision_Normal)) &#123;<span class="hljs-comment">//buffer mode not support Normal Precision yet</span><br>    permitFloat16 = <span class="hljs-literal">true</span>;<br>&#125;<br>mIsSupportedFP16 = mIsDeviceSupportedFP16 &amp;&amp; permitFloat16;<br><span class="hljs-built_in">MNN_PRINT</span>(<span class="hljs-string">&quot;opencl support fp16: %d, device support fp16: %d, permit fp16: %d\n&quot;</span>, mIsSupportedFP16, mIsDeviceSupportedFP16, permitFloat16);<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/03/11/hello-world/"/>
    <url>/2024/03/11/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
